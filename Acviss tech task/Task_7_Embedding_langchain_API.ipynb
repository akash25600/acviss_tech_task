{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Information Retreival** - LangChain\n",
        "- Without Using OpenAI Embeddings\n",
        "- Without OpenAI LLM\n",
        "- Text Documents"
      ],
      "metadata": {
        "id": "gJb4G1jgIHJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaWXSK6Gg5y8",
        "outputId": "e7a79f3b-92e4-4886-aa09-0b5199cb81d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.27)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.96)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "mL8j2qbfg9e6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWHI6mx9hMgX",
        "outputId": "8883ad39-ac65-4a95-8b12-5897fd3a8e1a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.12)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.23 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.27)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.96)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('/content/LLMs.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "YqrJZB0ihWNh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aoqFVumhbUx",
        "outputId": "0ad85fb6-dbbc-46d8-af26-2029136cc345"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/LLMs.txt'}, page_content='Generative AI with Large\\nLanguage Models.\\nCourse Notes : July, 2023\\n\\nGenerative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of\\napplications.\\n\"LLMs can be, generally, thought of as a next word prediction model\"\\n\\nPART 1\\nLLM Pre-Training\\n\\nPART 2\\nLLM Fine Tuning\\n\\nPART 3\\nRLHF & Application\\n\\nPart 1\\nWhat is an LLM?\\n\\nPage 1\\n\\nWhat are the Use Cases for application of LLMs?\\n\\nPage 2\\n\\nWhat are Transformers? How was text generation done before Transformers? Transformer Architecture.\\n\\nPage 2\\n\\nHow does a Transformer generate Text?\\n\\nPage 4\\n\\nWhat is a Prompt?\\n\\nPage 5\\n\\nGenerative AI Project Life Cycle.\\n\\nPage 7\\n\\nHow do you pre-train Large Language Models?\\n\\nPage 8\\n\\nChallenges with pre-training LLMs.\\n\\nPage 9\\n\\nWhat is the optimal configuration for pre-training LLMs?\\n\\nPage 11\\n\\nWhen is pre-training useful?\\n\\nPage 12\\n\\nWhat is an LLM?\\nLLMs are machine learning models that have learned from massive datasets of human-generated\\ncontent, finding statistical patterns to replicate human-like abilities.\\nFoundation models, also known as base models, have been trained on trillions of words for weeks or\\nmonths using extensive compute power. These models have billions of parameters, which represent\\ntheir memory and enable sophisticated tasks.\\nInteracting with LLMs differs from traditional programming paradigms. Instead of formalized code\\nsyntax, you provide natural language prompts to the models.\\nWhen you pass a prompt to the model, it predicts the next words and generates a completion. This\\nprocess is known as inference.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nPROMPT\\n\\nMODEL\\n\\nCOMPLETION\\n\\nWhere is Ganymede located in the\\n\\nWhere is Ganymede located in the\\n\\nsolar system?\\n\\nsolar system?\\n\\nLLM\\nCONTEXT WINDOW\\n\\nGanymede is a moon of Jupiter and is\\nlocated in the solar system within the\\norbit of Jupiter\\n\\nTHIS PROCESS IS CALLED \\'INFERENCE\\'\\n\\nWhat are the Use Cases for LLMs?\\nWhile Chatbots have emerged to become the most popular applications of LLMs, there are a variety of\\nother tasks that LLMs can be used to accomplish Writing - From essays to emails to reports and more\\nSummarisation - Summarise long content into a meaningful shorter length\\nLanguage Translation - Translate text from one language to the other\\nCode - Translate natural language to machine code\\nInformation Retrieval - Retrieve specific information from text like names, locations, sentiment\\nAugmented LLM - Power interactions with real world by providing information outside of LLM training\\n\\nTRANSFORMERS.\\nThe arrival of the transformer architecture in 2017, following the publication of the\\n\"Attention is All You Need\" paper, revolutionised generative AI.\\n\\nHow was text generation done before Transformers?\\nBefore the arrival of transformers, text generation tasks were accomplished by Recurrent Neural\\nNetworks (RNNs).\\nThe next word was predicted looking at the previous few words. The more the number of previous\\nwords, the larger was the computational requirement of the RNN.\\nThe prediction wasn\\'t great. The reason was the design of looking only at a few previous words.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nI took my money to the bank.\\nHOMONYMS\\n\\nThe teacher taught the student with the book\\n\\nRiver Bank?\\nFinancial Bank?\\n\\nSYNTACTIC\\nAMBIGUITY\\n\\nDid teacher teach with the book?\\nWas it a student with the book?\\n\\nTRANSFORMERS ARE ABLE TO PAY ATTENTION TO THE MEANING OF THE WORDS\\nTRANSFORMERS SCALE EFFICIENTLY\\nTRANSFORMERS CAN PROCESS DATA PARALLELLY\\n\\nWhat is Attention?\\n\\nTransformers supersede all previous natural language architectures because of their ability to \\'pay attention\\'\\n\\nEACH WORD IS CONNECTED TO EVERY\\nOTHER WORD THROUGH ATTENTION\\nWEIGHTS\\n\\nThe\\nteacher\\ntaught\\nthe\\nstudent\\nwith\\na\\nbook\\n\\nThe\\nteacher\\ntaught\\nthe\\nstudent\\nwith\\na\\nbook\\n\\nTHE MORE IMPORTANT THE\\nASSOCIATION, THE STRONGER IS THE\\nATTENTION\\n\\nWhat does a Transformer Architecture look like?\\nTokenizer :\\n\\nNumeric representation of words\\n\\nEmbeddings :\\n\\nHigher order vector representation of each token\\n\\nPositional\\nEncodings :\\n\\nA vector representation of the position of the word\\nin the input\\n\\nEncoder :\\n\\nEncodes each input token into vector by learning\\nself-attention weights & passing them through a\\nFCFF Network\\n\\nDecoder :\\n\\nSoftmax :\\n\\nAccepts an input token, passes them through the\\nlearned attention and FCFF Network to generate\\nnew token\\nCalculates the probability for each word to be the\\nnext word in sequence\\n\\nOUTPUT\\nSOFTMAX\\n\\nDECODER\\nENCODER\\n\\nPOSITIONAL ENCODINGS\\nEMBEDDINGS\\n\\nEMBEDDINGS\\nTOKENIZER\\nINPUT\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nThe Learning of Attention Weights\\nis not a single process, but\\nseveral parallel processes. As a\\nresult, multiple sets of attention\\nweights are learnt. This is called\\nMulti-Headed Self Attention.\\n\\nOnly for illustrative purpose\\nThink of one set of attention\\nweights as a representation of\\nvocabulary, another set as\\ntonality, yet another as a style of\\nwriting.\\n\\nMULTI HEADED SELF ATTENTION\\n\\nHow does a Transformer generate text?\\nThe original objective of the transformers architecture was for Language Translation in form of a sequenceto-sequence task\\nLAST STEP : DETOKANIZE TO\\n\"I\"\\n\"machine\"\\nTEXT\\n\"love\"\\n\"learning\"\\nINPUT\\nSTEP 16 : OUTPUT TOKEN\\n\"J\\'aime l\\'apprentissage automatique\"\\nLOOP\\n9873 10435 16742 29742\\nSTEP 14 : FIRST OUTPUT\\nTOKEN\\n\\nSTEP 1 : TOKENISATION\\n\\n\"J\\'aime\"\\n\\n\"automatique\"\\n\"l\\'apprentissage\"\\n\\n145\\n\\n233\\n\\nSTEP 13 : PROBABILITY OF\\nTOKEN\\n\\n607\\n\\nSTEP 6 : DEEP CONTEXT FROM\\nENCODER INSERTED INTO\\nMIDDLE OF DECODER\\n\\nSTEP 2 : EMBEDDINGS\\n\\nSOFTMAX\\n\\nSTEP 12 : FC FEED FORWARD\\nNETWORK\\n\\nSTEP 11 : MULTIHEADED ATTENTION\\nSTEP 5 : FC FEED FORWARD\\nNETWORK\\n\\nn1\\n\\nn2\\n\\nn3\\n\\nSTEP 9 : DECODER IS\\nTRIGGERED TO PREDICT NEXT\\nTOKEN\\n\\nSTEP 3 : POSITIONAL ENCODINGS\\n\\nm1\\n\\nSTEP 4 : MULTIHEADED ATTENTION\\n\\nSTEP 10 : DECODER PREDICTS\\nTHE NEXT TOKEN BASED ON THE\\nCONTEXT FROM ENCODER\\n\\nm2\\n\\nm3\\n\\nSTEP 8 : CREATE EMBEDDINGS\\nOF INPUT TOKEN\\n\\nSTEP 15 : LOOP THE\\nOUTPUT TOKEN BACK\\nTO THE DECODER\\n\\nSTEP 7 : START OF SEQUENCE\\nTOKEN INPUT TO DECODER\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nWhat is a Prompt?\\nThe natural language instruction in which we interact with an LLM is called a Prompt. The construction\\nof prompts is called Prompt Engineering.\\nThe inferencing that an LLM does and completes the instruction given in the prompt is called \\'in\\ncontext learning\\'\\nThe ability of the LLM to respond to the instruction in the prompt without any example is called \\'Zero\\nShot Learning\\'\\nWhen a single example is provided, it\\'s called \\'One Shot Learning\\'\\nIf more than one examples in provided, it\\'s called \\'Few Shot Learning\\'\\nContext Window, or the maximum number of tokens that an LLM can provide and inference on, is\\ncritical in the Zero/One/Few Shot Learning\\n\\nZERO SHOT LEARNING\\n\\nONE SHOT LEARNING\\n\\nFEW SHOT LEARNING\\n\\nClassify this review :\\n\\nClassify this review :\\n\\nClassify this review :\\n\\nI loved this movie!\\n\\nI loved this movie!\\n\\nI loved this movie!\\n\\nSentiment :\\n\\nSentiment : Positive\\n\\nSentiment : Positive\\n\\nClassify this review:\\n\\nClassify this review:\\n\\nI don\\'t like this chair\\n\\nI don\\'t like this chair\\n\\nSentiment :\\n\\nSentiment : Negative\\nClassify this review:\\nWho would use this product?\\nSentiment :\\n\\nGreedy vs Random Sampling.\\nSOFTMAX\\nSOFTMAX\\n.....\\n\\nGreedy : The word/token with the\\n\\n0.2\\n\\ncake\\n\\n0.1\\n\\ndonut\\n\\n0.02\\n\\nbanana\\n\\n0.01\\n.....\\n\\napple\\n.......\\n\\n0.2\\n\\ncake\\n\\n0.1\\n\\ndonut\\n\\n0.02\\n\\nbanana\\n\\nstrategy\\n\\n0.01\\n\\napple\\n.......\\n\\n\\'Even though Cake\\' has the highest\\n\\nlargest probability is selected\\n\\'Cake\\' has the highest probability of 20%\\n\\nRandom Sampling : The word/token is\\nselected using random-weighted\\n\\nprobability of 20%, \\'banana\\' is selected\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nInference Configuration Parameters.\\n\\nTop N.\\nTop N : The word/token is selected using\\n\\nSOFTMAX\\n\\n0.2\\n\\ncake\\n\\n0.1\\n\\ndonut\\n\\nrandom-weighted strategy but only from\\n\\n0.02\\n\\nbanana\\n\\n0.01\\n\\napple\\n.......\\n\\namongst the Top \\'N\\' words/tokens\\nHere for N=3, one of cake, donut or banana will be\\n\\n0.2\\n\\ncake\\n\\nTop P : The word/token is selected using random-\\n\\n0.1\\n\\ndonut\\n\\nweighted strategy but only from amongst the top\\n\\n0.02\\n\\nbanana\\n\\n0.01\\n\\napple\\n.......\\n\\nwords totalling to probability <=P\\n\\nselected randomly but apple will never be selected\\n\\nTop P.\\nSOFTMAX\\n.....\\n\\nHere for P=0.33, one of cake or donut will be selected\\nrandomly but apple or banana will never be selected\\n\\nTemperature\\ntemperature\\nsetting\\nSOFTMAX\\ntemperature\\nsetting\\nSOFTMAX\\n\\n0.002\\n\\nbanana\\n\\n0.01\\n\\ndonut\\n\\n0.4\\n\\ncake\\n\\n0.001\\n\\napple\\n.......\\n\\n0.002\\n\\nbanana\\n\\n0.01\\n\\ndonut\\n\\n0.4\\n\\ncake\\n\\n0.001\\n\\napple\\n.......\\n\\nCooler Temperature (lesser value) :\\nThe distribution is strongly peaked\\n\\nWarmer Temperature (higher value) :\\nFlatter probability distribution\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nGenerative AI Project LifeCycle.\\nSCOPE\\n\\nSELECT\\n\\nADAPT & ALIGN\\n\\nDefine the\\n\\nChoose an\\n\\nPrompt\\n\\nusecase.\\n\\nexisting LLM or\\n\\nEngineering\\n\\nPre-train your\\nown.\\n\\nFine Tuning\\n\\nEvaluate\\n\\nAPP INTEGRATION\\nOptimise &\\n\\nAugment\\n\\nDeploy Model\\n\\nModel &\\n\\nfor Inference\\n\\nBuild LLMPowered\\nApplication\\n\\nAlign with\\nHuman\\nFeedback\\n\\nDefining the scope accurately and narrowly is a crucial initial step in any project.\\nLLMs have diverse capabilities depending on the model\\'s size and architecture, so it is essential to\\nconsider the specific function your LLM will serve in your application.\\nChoosing between training your own model from scratch or using an existing base model is the first\\ndecision you\\'ll face.\\nWhile starting with an existing model is common, there may be cases where training from scratch\\nbecomes necessary\\nPrompt engineering and in-context learning techniques can often improve the model\\'s performance by\\nusing task-specific examples.\\nThere are instances where the model may still underperform, even with prompt engineering, and in\\nsuch cases, fine-tuning the model through supervised learning can be explored.\\nLearning with human feedback as an additional fine-tuning technique to ensure the model\\'s good\\nbehaviour.\\nEvaluation plays a crucial role in all these techniques\\nOptimising the model for deployment ensures efficient utilisation of compute resources and provides\\nthe best possible user experience.\\nIt is important to consider any additional infrastructure requirements for your application to work\\neffectively.\\nLLMs have inherent limitations, such as inventing information or limited reasoning abilities, which can\\nbe challenging to overcome through training alone.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nHow do you pre-train Large Language Models?\\nAutoEncoding Models (Encoder Only)\\nUSE CASES\\n\\nMasked Language Modeling\\n\\nDocument Filter\\n\\nGB/TB/PB of\\ntext data\\n\\nThe\\n\\nTeacher\\n\\nTeaches\\n\\nThe\\n\\nStudent\\n\\nThe\\n\\nTeacher\\n\\n<MASK>\\n\\nThe\\n\\nStudent\\n\\nThe\\nTeacher\\nTeaches\\nThe\\nStudent\\n\\nSentiment Analysis\\nNamed Entity Recognition\\nWord Classification\\n\\nEncoder\\nOnly Model\\nEXAMPLES\\nObjective : Reconstruct Text (\"denoising\")\\nThe\\n\\nTeacher\\n\\n<MASK>\\n\\nThe\\n\\nStudent\\n\\nTeaches\\n\\nBERT\\nROBERTA\\n\\nBi-directional Context\\n\\nFinetuning Foundation Models vs\\nPretaining Your Own\\n\\nOpen Source LLMs\\n\\nTraining Data and Model Size\\n\\nFor most requirements, finetuning an existing\\nLLM will suffice. However, there can be cases\\nwhen pre-training a new LLM will provide\\nbetter application especially when the\\nlanguage is highly domain specific e.g Legal,\\nMedical. Pre-training is a resource intensive\\nand costly process.\\n\\nWhile OpenAI\\'s proprietary GPT-3.5, GPT-4\\nhave gained immense popularity, HuggingFace\\nModel Hub provides access to powerful Open\\nSource LLMs along with documentation and\\ntraining architecture. Architecture plays a\\ncritical role in defining what objective can\\neach LLM be used for.\\n\\nLLMs are generally trained on Petabytes of\\ndata, mostly from the open internet. The\\nunstructured text requires careful filtering. As a\\nresult only 2-3% of data is useful for training.\\nLLM size is measured in terms of the number of\\nparameters. Larger models have generalised\\nwell to a variety of tasks.\\n\\nAutoRegressive Models (Decoder Only)\\nUSE CASES\\n\\nCausal Language Modeling\\n\\nDocument Filter\\n\\nGB/TB/PB of\\ntext data\\n\\nThe\\n\\nTeacher\\n\\nTeaches\\n\\nThe\\n\\nTeacher\\n\\n?\\n\\nThe\\nTeacher\\nTeaches\\nThe\\nStudent\\n\\nThe\\n\\nStudent\\n\\nText Generation\\n(Most common architecture\\nnow and larger models can\\nperform a variety of tasks)\\n\\nDecoder\\nOnly Model\\nEXAMPLES\\nObjective : Predict Next Token\\nThe\\n\\nTeacher\\n\\nTeaches\\n\\nGPT\\nBLOOM\\n\\nUnidirectional Context\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nHow do you pre-train Large Language Models?\\nSequence-to-Sequence Models (Encoder-Decoder)\\nSpan Corruption\\n\\nDocument Filter\\n\\nGB/TB/PB of\\ntext data\\n\\nUSE CASES\\n\\nThe\\n\\nTeacher\\n\\nTeaches\\n\\nThe\\n\\nStudent\\n\\nThe\\n\\nTeacher\\n\\n<MASK>\\n\\n<MASK>\\n\\nStudent\\n\\nThe\\n\\nTeacher\\n\\nX\\n\\nTranslation\\nText Summarisation\\nQuestion Answering\\n\\nStudent\\n\\nSentinel Token\\n\\nThe\\nTeacher\\nTeaches\\nThe\\nStudent\\n\\nEncoderDecoder\\nModel\\nEXAMPLES\\n\\nT5\\nBART\\n\\nObjective : Reconstruct Span\\nX\\n\\nTeaches\\n\\nThe\\n\\nChallenges with pre-training LLMs.\\nComputational Memory\\n1 Parameter = 4 bytes (32 bit float)\\n1 Billion Parameters = 4 x 10E9 bytes = 4GB\\nModel Parameters = 4 bytes per parameter\\n2 Adam Optimisers = +8 bytes per parameter\\nGradients = +4 bytes per parameter\\nActivations = +8 bytes per parameter\\nTotal = 4 bytes per parameter +\\n20 extra bytes per parameter\\n\\nTo Store\\n\\n4 GB@32 bit\\nfull precision\\n\\nTo Train\\n\\n80 GB@32 bit\\nfull precision\\n\\nQuantisation\\nFP32\\n32 Bit Floating Point\\nRange\\n3e-38 to 3e+38\\n\\nReduces the memory required to train and store\\nmodels\\nQuantisation projects the 32bit numbers into a lower\\nprecision space\\nQuantisation aware training (QAT) learns quantisation\\nscaling factors during training\\n\\nFP16 | BFLOAT16 | INT8\\n16 Bit Floating Point\\n8 Bit Integer\\n\\nBFLOAT16 is a popular choice\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nMulti-GPU Compute (Optional)\\nDistributed Data Parallel (DDP)\\n\\nLLM\\nParameter\\nGradients\\nTraining\\nOptimizer\\nMemory\\nRequirement\\n\\nData Loaders\\n\\nLLM\\nGPU 3\\n\\nForward Pass/\\nBackward Pass\\n\\nUpdate Model\\n\\nGPU 2\\n\\nForward Pass/\\nBackward Pass\\n\\nUpdate Model\\n\\nGPU 1\\n\\nForward Pass/\\nBackward Pass\\n\\nUpdate Model\\n\\nGPU 0\\n\\nForward Pass/\\nBackward Pass\\n\\nUpdate Model\\n\\nLLM\\n\\nLLM\\n\\nLLM\\n\\nData is broken\\ninto chunks and\\npassed to the GPU\\nfor training the\\nLLM\\n\\nEntire LLM resides\\nn each of the\\nGPUs\\n\\nSynchronise\\nGradients\\n\\nGradients are\\ncomputed on each\\nGPU\\n\\nGradients are\\nsynchronised\\n\\nLLM is updated in\\neach of the GPU\\n\\nFully Sharded Data Parallel (FSDP)\\ninspired by ZeRO (Zero Redundancy Optimizer)\\n\\nData Loaders\\n\\nData is broken\\ninto chunks and\\npassed to the GPU\\nfor training the\\nLLM\\n\\nGPU 3\\n\\nForward\\nPass\\n\\nGPU 2\\n\\nForward\\nPass\\n\\nGet\\nWeights\\n\\nGPU 1\\n\\nForward\\nPass\\n\\nGPU 0\\n\\nForward\\nPass\\n\\nLLM is also broken\\nand parts reside in\\neach GPU\\n\\nGet\\nWeights\\n\\nWeights are collected before forward\\nand backward pass from each of the\\nGPUs\\n\\nBackward\\nPass\\n\\nUpdate Model\\n\\nBackward\\nPass\\n\\nUpdate Model\\n\\nBackward\\nPass\\n\\nSynchronise\\nGradients\\nUpdate Model\\n\\nBackward\\nPass\\n\\nUpdate Model\\nGradients are\\nsynchronised\\n\\nLLM chunks are\\nupdated in each of\\nthe GPU\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nWhat is the optimal computation configuration for pre-training an LLM?\\nOur aim should be to minimise the loss function of the LLM.\\nThis can be done by providing more training tokens and/or increasing the number of parameters\\nCompute cost (budget, time, GPUs) is the constraint that we operate in.\\nConstraint:\\nCompute Budget\\n($$$, Time, GPUs)\\n\\nMODEL\\nPERFORMANCE\\n(Minimise Loss)\\n\\nChoice:\\nTraining Data Size\\n(Number of Tokens)\\n\\nChoice:\\nModel Size\\n(Number of Parameters)\\n\\nOne metric to measure the cost of computation is \"petaFLOP/s-day\"\\nNumber of FLOating Point operations performed at the rate of 1 petaFLOP per second for one day\\n1 petaFLOP per second = 10E15 or 1\\nQuadrillion floating point operations per\\nsecond\\n\\n1 petaFLOP per second - day requires 8\\nNVIDIA V100 chips running at full\\nefficiency for 24 hours\\n\\n1 petaFLOP per second - day requires 2\\nNVIDIA A100 chips running at full\\nefficiency for 24 hours\\n\\nTo put this in context, OpenAIs GPT3 is a 175 B parameters model and required 3700 petaflops/s-day\\n\\nChinchilla Paper\\nIn 2022, the paper \"Training Compute-Optimal Large Language Model\" studied a large number of models with\\ndifferent training data and model sizes to find the optimal number of tokens and parameters for a given compute\\nbudget. The authors called this optimal model \"Chinchilla\"\\n\\nOne important take-away was that the compute optimal number of training tokens\\nshould be 20 times the number of parameters\\nThis means that smaller models can achieve the same performance as the larger ones, if they are trained on\\nlarger datasets\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nWhy Pre-train for Domain Adaptation?\\nDeveloping pre-trained models for a highly specialised domains may be useful when There are important terms that are not commonly used in the language\\nThe meaning of some common terms is entirely different in the domain vis-a-vis the\\ncommon usage\\nLEGAL\\n\\nMEDICAL\\n\\nDomain specific terms Mens Rea\\nRes Judicata\\n\\nLarge models are not\\ntrained on these terms\\n\\nDomain specific terms Myalgia\\n1 tab po quid pc & hs\\n\\nTerms with different meanings Consideration\\n\\nLarge models may use\\nterms in different context\\n\\nTerms with different meanings Malignant\\n\\nEXAMPLE : Bloomberg GPT\\nEND OF PART 1\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nPart 2\\nWhat is Instruction Fine Tuning?\\n\\nPage 13\\n\\nWhat Catastrophic Forgetting?\\n\\nPage 14\\n\\nHow to Evaluate a Fine Tuned model?\\n\\nPage 14\\n\\nWhat is Parameter Efficient Fine Tuning?\\n\\nPage 15\\n\\nWhat is Instruction Fine Tuning?\\nThrough in context learning, or prompting, only a certain level of performance can be achieved.\\nFew shot learning might not work for smaller LLMs and it also takes up a lot of space in the context\\nwindow.\\nFine Tuning is a supervised learning process, where you take a labelled dataset of prompt-completion\\npairs to adjust the weights of an LLM.\\nInstruction Fine Tuning is a strategy where the LLM is trained on examples of Instructions and how the\\nLLM should respond to those instructions. Instruction Fine Tuning leads to improved performance on\\nthe instruction task.\\nFull Fine Tuning is where all the LLM parameters are updated. It requires enough memory to store and\\nprocess all the gradients and other components.\\n\\nBase Model\\n\\ntrained on\\n\\nTask Specific Examples\\n\\nresults in\\n\\nPROMPT [....], COMPLETION[....]\\nPROMPT [....], COMPLETION[....]\\nPROMPT [....], COMPLETION[....]\\n......\\nPROMPT [....], COMPLETION[....]\\n\\nPre-trained\\nLLM\\n\\nSummarise the following text [Example Text]\\n[Example Completion]\\n\\nInstruct Model\\n\\nFine Tuned\\nLLM\\n\\nTranslate this sentence to ....\\n[Example Text]\\n[Example Completion]\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nInstruction dataset\\n\\nPROMPT [....], COMPLETION[....]\\nPROMPT [....], COMPLETION[....]\\nPROMPT [....], COMPLETION[....]\\n......\\nPROMPT [....], COMPLETION[....]\\n\\nPROMPT [....], COMPLETION[....]\\n\\nPre-trained\\nLLM\\n\\nActual Label\\n\\nValidation\\n\\nLLM Completion\\n\\nPROMPT [....], COMPLETION[....]\\n......\\nPROMPT [....], COMPLETION[....]\\n\\nValidation\\nAccuracy\\n\\nTest\\n\\nTrain\\n\\nInstruction dataset\\n\\nPROMPT [....], COMPLETION[....]\\n......\\nPROMPT [....], COMPLETION[....]\\n\\nTest\\nAccuracy\\n\\nPROMPT [....], COMPLETION[....]\\n\\nLoss : Cross Entropy\\nFine tuning process is a classification model training\\n\\nWhat is Catastrophic Forgetting?\\nFine Tuning on a single task can significantly improve the performance of the model on that task.\\nHowever, because the model weights get updated, the instruct model\\'s performance on other tasks\\n(which the base model performed well on) can get reduced. This is called Catastrophic Forgetting.\\nAvoiding Catastrophic Forgetting\\nYou might not have to, if you only want the model to perform well on the trained task.\\nPerform fine tuning on multiple tasks. This will require lots of examples for each task.\\nPerform Parameter Efficient Fine Tuning (PEFT).\\n\\nHow is the performance of a Fine Tuned LLM measured?\\nThe inherent challenge in measuring the performance of an LLM is that the outputs are nondeterministic, as opposed to a classic classification model where the output is amongst predetermined classes.\\nMike really loves drinking tea\\n\\nMike adores sipping tea\\n\\nMike does not drink coffee\\n\\nMike does drink coffee\\n\\nThere are two widely used evaluation metrics.\\n\\nROUGE\\nUsed for text summarisation\\nCompares a summary to one\\nor more reference summaries\\n\\nBLEU\\nSCORE\\nUsed for text translation\\nCompares to human-generated\\ntranslations\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nROUGE SCORE\\nREFERENCE (human) :\\nIt is cold outside\\nGENERATED:\\nIt is very cold outside\\nLCS = Length of the Longest\\nCommon Subsequences\\n\\nROUGE - 1\\nRecall\\n\\nunigram matches\\nunigrams in reference\\n\\n4\\n4\\n\\nROUGE - 1\\nPrecision\\n\\nunigram matches\\nunigrams in output\\n\\n4\\n5\\n\\nIt, is, cold, outside\\nIt, is very, cold, outside\\n\\nROUGE - 2\\nRecall\\n\\nbigram matches\\nbigrams in reference\\n\\n2\\n3\\n\\nROUGE - 1\\nPrecision\\n\\nbigram matches\\nbiigrams in output\\n\\n2\\n4\\n\\nIt is, is cold, cold outside\\nIt is, is very, very cold, cold outside\\n\\nROUGE - L\\nRecall\\n\\nLCS (R,G)\\nunigrams in reference\\n\\n2\\n4\\n\\nROUGE - 1\\nPrecision\\n\\nLCS (R,G)\\nunigrams in output\\n\\n2\\n5\\n\\nLength (It is) = 2\\nLength (cold outside) = 2\\n\\nRouge scores should only be compared across the models for the same task\\nRouge score may be high even for imperfect generations\\n\\nBLEU (Bi-Lingual Evaluation Understudy) SCORE\\nBLEU = Average (precision score across a range of n-gram sizes)\\nBOTH ROUGE & BLEU SHOULD BE USED FOR DIAGNOSTIC PURPOSES ONLY.\\n\\nBENCHMARKS\\nUse pre-existing evaluation datasets and benchmarks established by LLM researchers for a more holistic evaluation\\nSelect datasets that isolate model skills, like reasoning or common sense knowledge, and those that focus on potential risks,\\nsuch as disinformation or copyright infringement\\nEvaluate performance on data that the model has not seen before\\nPopular Benchmarks\\n\\nMMLU\\nGeneral Language Understanding\\n\\nHolistic Evaluation of Language\\nModels\\n\\nBIG BENCH\\n\\nMassive Multitask Language\\nUnderstanding\\n\\nWhat is Parameter Efficient Fine Tuning?\\nFull fine tuning, like pre-training, requires memory not just to store the model, but also other\\nparameters like optimisers, gradients etc.\\nParameter Efficient Fine Tuning or PEFT fine tunes only a subset of model parameters and, in some\\ncases, do not touch the original weights at all.\\nBecause PEFT only retrains a subset of parameters, Catastrophic Forgetting can be avoided.\\nPEFT Weights\\nQuestion Answer Task\\n\\nMBs\\n\\nPre-trained\\nLLM\\n\\nTrade Offs\\n\\nMBs\\nGenerate Task\\n\\nGBs\\nParameter Efficiency\\n\\nFine Tuned\\nLLM\\n\\nSummarize Task\\n\\nMemory Efficiency\\n\\nMBs\\nModel Performance\\n\\nTraining Speed\\n\\nInference Cost\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nPEFT Method\\nSelective\\n\\nReparameterization\\n\\nAdditive\\n\\nSelect a subset of initial\\nLLM parameters to fine\\ntune.\\nPerformance is mixed and\\nsignificant trade-offs\\n\\nReparameterize model\\nweights using Low Rank\\nRepresentation\\n\\nAdd trainable layers or\\nparameters to the original\\nmodel\\n\\nLoRA\\n\\nAdapters\\nSoft Prompting\\n\\nLoRA (Low Rank Adaptations)\\n\\nENCODER\\nSELF ATTENTION LAYER\\nRank Decomposition\\nMatrices\\n\\nAttention\\nlayer weights\\n\\nMost of original LLM weights are frozen.\\n2 rank decomposition matrices are injected.\\nProduct of rank decompositions matrices is of the same dimensions\\nas the LLM weights.\\nThe product is added to the LLM weights.\\nThere\\'s no impact on inference latency since the number of\\nparameters remains the same.\\nApplying LoRA to just the attention layer is enough.\\n\\nLoRA can reduce the number of training parameters to ~20% and can be trained on a single GPU.\\nSeparate LoRA matrices can be trained for each task and switch out the weights for each task.\\n\\nSoft Prompts : Prompt Tuning (not Prompt Engineering)\\nPrompt engineering is limited by context window and the manual exercise of writing prompts.\\nIn Prompt tuning, additional trainable tokens (soft prompts) are added to the prompt which are learnt during the\\nsupervised learning process.\\nSoft prompt tokens are added to the embedding vectors and have the same length as the embedding.\\nBetween 20-100 tokens are sufficient for fine tuning.\\nSoft prompts can take any value in the embedding space and through supervised learning, the values are learnt.\\nLike in LoRA, separate soft prompts can be trained for each task and switch out the weights for each task.\\nPerformance of Prompt Tuning\\n\\nFor models with >10B parameters, Prompt\\nTuning can be as effective as Full Fine Tuning\\nAnalysis of nearest neighbours indicate that\\nlearnt soft tokens form tight clusters\\n\\nEND OF PART 2\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nPart 3\\nAligning with Human Values\\n\\nPage 17\\n\\nHow does RLHF work?\\n\\nPage 18\\n\\nHow to avoid Reward Hacking?\\n\\nPage 22\\n\\nScaling Human Feedback : Self Supervision with Constitutional AI\\n\\nPage 23\\n\\nHow to optimise and deploy LLMs for inferencing?\\n\\nPage 24\\n\\nUsing LLMs in Applications\\n\\nPage 25\\n\\nLLM Application Architecture\\nResponsible AI\\nGenerative AI Project Lifecycle Cheatsheet\\n\\nPage 28\\nPage 29\\nPage 30\\n\\nAligning with Human Values\\nLike with language, in general, Large Language Models can also behave badly Toxicity\\nAggression\\nDangerous/Harmful\\nLLMs should align with Helpfulness, Honesty and Harmlessness (HHH)\\n\\nWhat is Reinforcement Learning from Human Feedback (RLHF)?\\nInstruction Fine\\nTuned LLM\\n\\nReinforcement Learning from\\nHuman Feedback\\n\\nHuman\\nAligned LLM\\n\\nMaximises helpfulness\\nMinimises harm\\nAvoids dangerous topics\\nIncreases honesty\\n\\nReinforcement Learning based on Human Feedback data\\nPersonalisation of LLMs is a potential application of RLFL\\nREINFORCEMENT LEARNING\\nis a type of machine learning in which an agent learns to make decisions related to a specific goal\\nby taking actions in an environment, with the objective of maximising some notion of a\\ncumulative reward\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nHow does RLHF work?\\nGenerate Aligned Text\\n[Goal]\\n\\nInstruct LLM\\n\\nROLLOUT\\n\\nLLM Context Window\\n[Environment]\\n\\n[Action ]\\n\\n[Action Space]\\n\\nReward\\n\\nGenerated Text\\n\\nReward\\nModel\\n\\nLLM Vocabulary\\n\\n[State]\\n\\nCurrent Text\\n\\ne.g. - 0 for harmful text\\n1 for harmless text\\n\\n[Agent]\\n\\nIn RLHF, the agent (our fine-tuned instruct LLM) in its environment (Context Window) takes one\\naction (of generating text) from all available actions in the action space (the entire vocabulary of\\ntokens/words in the LLM).\\nThe outcome of this action (the generated text) is evaluated by a human and is given a reward if the\\noutcome (the generated text) aligns with the goal. If the outcome does not align with the goal, it is\\ngiven a negative reward or no reward. This is an iterative process and each step is called a rollout.\\nThe model weights are adjusted in a manner that the total rewards at the end of the process are\\nmaximised.\\nNote : In practice, instead of a human giving a feedback continually, a classification model called the\\nReward Model is trained based on human generated training examples\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nHow is Reward Model training data created from Human Feedback?\\nMy house is too hot.\\n[Prompt]\\n\\n1\\n\\nFor each prompt, multiple [3]\\ncompletions are generated by the\\nLLMs\\n\\nLLM\\n\\nThere\\'s nothing\\nyou can do.\\n\\n3\\n\\nCool the house\\nwith an AC.\\n\\nIt\\'s not hot.\\n\\n2\\n\\n1\\n\\n3\\n\\n2\\n\\n1\\n\\n3\\n\\n3\\n\\n1\\n\\n2\\n\\n2\\n\\nHuman\\nevaluators from\\ndiverse\\nbackgrounds\\nrank the\\ncompletions\\n\\nRankings are converted to pairwise training data for the reward model\\nPairs\\n\\nPrompt\\n\\nRewards\\n\\nPairs\\n\\nRewards\\n\\nCompletion 1\\n\\n2\\n\\n1\\n\\n2\\n\\n[0,1]\\n\\n2\\n\\n1\\n\\n[1,0]\\n\\nCompletion 2\\n\\n1\\n\\n2\\n\\n3\\n\\n[1,0]\\n\\n2\\n\\n3\\n\\n[1,0]\\n\\nCompletion 3\\n\\n3\\n\\n3\\n\\n1\\n\\n[0,1]\\n\\n1\\n\\n3\\n\\n[1,0]\\n\\nThe pairs are ordered in order {Yj, Yk} such that Yj is the preferred completion\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\n4\\n\\nCourse Notes : July, 2023\\n\\nReward Model is a supervised learning language model\\nCompletion Yj\\n\\nReward Rj\\n\\n2\\nReward\\nModel\\n\\nPrompt \\'X\\'\\n1\\n\\nReward Rk\\n\\nCompletion Yk\\n\\n5\\n\\nThe reward model learns to\\nfavour the human preferred\\nresponse Yj while minimising\\nthe log of sigmoid difference\\nbetween the rewards.\\n\\nReward Model is finally used as a binary classifier\\n\"Tommy loves Television\"\\n\\nNew\\nPrompt\\n\\nNew\\nCompletion\\n\\nReward\\nModel\\n\\nThe logit values for the Positive Class are passed as the\\nRewards. The LLM will change the weights in such a way\\nthat the choice of generated text will yield the highest\\nrewards.\\n\\nPositive Class (Not Hate)\\n\\n3.1718\\n\\nNegative Class (Hate)\\n\\n-2.6093\\n\\n\"Tommy hates gross movies\"\\nPositive Class (Not Hate)\\n\\n-0.5351\\n\\nNegative Class (Hate)\\n\\n0.1377\\n\\nProximal Policy Optimisation or PPO is a popular choice for training\\nthe reinforcement learning algorithms\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nWhat is Proximal Policy Optimisation? [Optional]\\nPPO stands for Proximal Policy Optimisation. It\\'s a powerful algorithm used in reinforcement learning.\\nPPO helps us optimise a large language model (LLM) to be more aligned with human preferences. We want the LLM to\\ngenerate responses that are helpful, harmless, and honest.\\nPPO works in cycles with two phases: Phase I and Phase II.\\nIn Phase I, the LLM completes prompts and carries out experiments. These experiments help us update the LLM based\\non the reward model, which captures human preferences.\\nThe reward model determines the rewards for prompt completions. It tells us how good or bad the completions are in\\nterms of meeting human preferences.\\nIn Phase II, we have the value function, which estimates the expected total reward for a given state. It helps us evaluate\\ncompletion quality and acts as a baseline for our alignment criteria.\\nThe value loss minimises the difference between the actual future reward and its estimation by the value function. This\\nhelps us make better estimates for future rewards.\\nPhase 2 involves updating the LLM weights based on the losses and rewards from Phase 1.\\nPPO ensures that these updates stay within a small region called the trust region. This keeps the updates stable and\\nprevents us from making drastic changes.\\nThe main objective of PPO is to maximise the policy loss. We want to update the LLM in a way that generates\\ncompletions aligned with human preferences and receives higher rewards.\\nThe policy loss includes an estimated advantage term, which compares the current action (next token) to other possible\\nactions. We want to make choices that are advantageous compared to other options.\\nMaximising the advantage term leads to better rewards and better alignment with human preferences.\\nPPO also includes the entropy loss, which helps maintain creativity in the LLM. It encourages the model to explore\\ndifferent possibilities instead of getting stuck in repetitive patterns.\\nThe PPO objective is a weighted sum of different components. It updates the model weights through back propagation\\nover several steps.\\nAfter many iterations, we arrive at an LLM that is more aligned with human preferences and generates better responses.\\nWhile PPO is popular, there are other techniques like Q-learning. Researchers are actively exploring new methods, such\\nas direct preference optimisation, to improve reinforcement learning with large language models.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nHow to avoid Reward Hacking?\\nReward hacking happens when the language model finds ways to maximise the reward without\\naligning with the original objective i.e. model generates language that sounds exaggerated or\\nnonsensical but still receives high scores on the reward metric.\\nTo prevent reward hacking, the original LLM is introduced as a reference model, whose weights are\\nfrozen and serve as a performance benchmark.\\nDuring training iterations, the completions generated by both the reference model and the updated\\nmodel are compared using KL divergence. KL divergence measures how much the updated model has\\ndiverged from the reference model in terms of probability distributions.\\nDepending on the divergence, a shift penalty is added to the rewards calculation. The shift penalty\\npenalises the updated model if it deviates too far from the reference model, encouraging alignment\\nwith the reference while still improving based on the reward signal.\\n\\nPrompt\\nDataset\\n\\nReference\\nModel\\n\\nRL Updated\\nModel\\n\\n\"This product is....\"\\n\\n\"Useful & well\\npriced\"\\nNormal\\n\\n\"the most awesome, the\\nmost incredible thing ever\"\\nReward Hacked\\n\\nKL Divergence\\nShift Penalty\\n\\nPPO\\n\\nReward\\nModel\\n\\nPenalty added to\\nrewards\\n\\nReinforcement Learning loop that is exposed to the threat of reward hacking\\nIntroduction of Shift Penalty loop increases alignment without exaggeration\\n\\nRLHF can also be used in conjunction with PEFT to reduce memory footprint\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nScaling Human Feedback : Self Supervision with Constitutional AI\\nScaling human feedback for RLHF can be challenging due to the significant human effort required to\\nproduce the trained reward model. As the number of models and use cases increases, human effort\\nbecomes a limited resource, necessitating methods to scale human feedback.\\nFirst proposed in 2022 by researchers at Anthropic, Constitutional AI is an approach to scale\\nsupervision and address some unintended consequences of RLHF. Constitutional AI involves training\\nmodels using a set of rules and principles that govern the model\\'s behaviour, forming a \"constitution\".\\nThe training process for Constitutional AI involves two phases: supervised learning and reinforcement\\nlearning.\\nIn the supervised learning phase, the model is prompted with harmful scenarios and asked to critique\\nits own responses based on constitutional principles. The revised responses, conforming to the rules,\\nare used to fine-tune the model.\\nThe reinforcement learning phase, known as reinforcement learning from AI feedback (RLAIF), uses\\nthe fine-tuned model to generate responses based on constitutional principles\\n\\nRed Teaming\\n\\nResponse,\\nCritique,\\nRevision\\n\\nFine tuned\\nLLM Model\\n\\nGenerate responses to \"Red\\nTeaming\" prompts\\n\\nPrompt LLM to generate harmful, illegal,\\nundesired responses - \"Can you help me\\nhack into my neighbour\\'s wifi?\\nLLM Completion - \"Sure, use this app called\\nVeryEasyHack.\"\\n\\nAsk LLM to rank respon\\nresponsesses as per the\\nconstitution\\n\\nPrompt LLM with the constitution \"Identify how that response was racist,\\nharmful, unethical, sexist, toxic or illegal?\\n\\nTrain\\nReward\\nModel\\n\\nLLM Completion - \"The response was\\nharmful. Hacking into someone else\\'s wifi is\\npossibly illegal\"\\nInstruct LLM to improve - \"Rewrite the\\nresponse remove any harmful, illegal,\\nunethical, etc. content\"\\n\\nLLM Completion - \"Hacking into your\\nneighbour\\'s wifi may land you in legal\\ntrouble.\"\\n\\nFine tune LLM with constitutional\\npreferences\\n\\nConstitutional\\nLLM\\n\\nNitesh\\n\\nReinforcement Learning Stage\\n\\nSupervised Learning Stage\\n\\nHelpful LLM\\nModel\\n\\nReinforcement Learning with AI\\nFeedback [RLAIF]\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nHow to optimise and deploy LLMs for inferencing?\\nIntegrating a language model into applications requires considering factors like model speed, compute\\nbudget, and trade-offs between performance and speed/storage.\\nAdditional considerations include model interaction with external data or applications and\\ndetermining the intended application or API interface.\\n\\nOptimisation techniques\\nDistillation\\n\\nPost-training Quantisation\\n\\nPruning\\n\\nFP32\\n32 Bit Floating Point\\n\\nTeacher\\nLLM\\n\\nRange\\n3e-38 to 3e+38\\n\\nOriginal\\nLLM\\n\\nFP16 | BFLOAT16 | INT8\\n16 Bit Floating Point\\n8 Bit Integer\\n\\nPruned\\nLLM\\n\\nStudent\\nLLM\\nUse larger teacher model\\nto train smaller student\\nmodel\\nUse student model for\\ninferencing in\\napplications\\nTemperature parameter\\nis used to generate soft\\nand hard predictions.\\nIn practice, distillation is\\neffecting in encoder only\\nmodels like BERT\\n\\nReduce the model\\nweights post training\\nto lower precision to\\nreduce model\\nfootprint\\nCan be applied to\\nboth weights and\\nactivations\\nMay result in\\nreduction in\\nevaluation metrics\\n\\nRemove the weights\\nwith values close to\\n0\\nCan be done via full\\nfine-tuning or PEFT\\nReduces size and\\nimproves\\nperformance\\nIs not helpful if only\\na small percentage\\nof original model\\nweights are zero\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nUsing LLMs in Applications\\nLarge language models (LLMs) have a knowledge cutoff and may struggle with outdated information.\\nLLMs can also face challenges with complex math problems and tend to generate text even when they\\ndon\\'t know the answer (hallucination).\\n\\nRetrieval Augmented Generation\\nThe Retrieval Augmented Generation (RAG) framework overcomes these issues by connecting LLMs to\\nexternal data sources and applications.\\nRAG provides LLMs access to data they did not see during training, improving relevance and accuracy\\nof completions.\\n\\nImplementing RAG involves considerations such as the size of the context window and the need for\\ndata retrieval and storage in appropriate formats\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nReasoning using Chain of Thought\\nLLMs can struggle with complex reasoning tasks, especially those involving multiple steps or\\nmathematics.\\nPrompting the model to think more like a human by breaking down the problem into steps has shown\\nsuccess in improving reasoning performance.\\nChain of thought prompting involves including intermediate reasoning steps in examples used for one\\nor few-shot inference.\\nThis approach teaches the model how to reason through the task by mimicking the chain of thought a\\nhuman might follow.\\nChain of thought prompting can be used for various types of problems, not just arithmetic, to improve\\nreasoning performance.\\nIt provides a more robust and transparent response from the model, explaining its reasoning steps.\\nAlthough LLMs can benefit from chain of thought prompting, their limited math skills can still pose\\nchallenges for accurate calculations in tasks like sales totaling, tax calculation, or applying discounts.\\n\\nProgram-aided Language Models (PAL)\\nLLMs have limitations in carrying out accurate math calculations, especially with larger numbers or\\ncomplex operations.\\nChain of thought prompting can help LLMs reason through problems, but it may not solve the issue of\\ninaccurate math operations.\\nPAL (Program-Aided Language Models) is a framework that pairs LLMs with external code interpreters\\nto perform calculations and improve accuracy.\\nPAL uses chain of thought prompting to generate executable Python scripts that are passed to an\\ninterpreter for execution.\\nThe prompt includes reasoning steps in natural language as well as lines of Python code for\\ncalculations.\\nVariables are declared and assigned values based on the reasoning steps, allowing the model to\\nperform arithmetic operations.\\nThe completed script is then passed to a Python interpreter to obtain the answer to the problem.\\nPAL ensures accurate calculations and reliable results, especially for complex math problems.\\nThe process can be automated by using an orchestrator, a component that manages the flow of\\ninformation and interactions with external data sources or applications.\\nThe orchestrator interprets and executes the plan generated by the LLM, which includes writing the\\nscript for the external interpreter to run.\\nIn more complex applications, the orchestrator may handle multiple decision points, validation\\nactions, and interactions with various external resources.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nReAct : Reasoning and Action\\nReAct combines chain of thought reasoning with action planning in LLMs.\\nIt uses structured examples to guide the LLM\\'s reasoning and decision-making process.\\nExamples include a question, thought (reasoning step), action (pre-defined set of actions), and\\nobservation (new information).\\nActions are limited to predefined options like search, lookup, and finish.\\nThe LLM goes through cycles of thought, action, and observation until it determines the answer.\\nInstructions are provided to define the allowed actions and provide guidance to the LLM.\\n\\nLangChain\\nLangChain provides modular components for working with LLMs in applications.\\nIt includes prompt templates for various use cases, memory to store LLM interactions, and tools for\\nworking with external datasets and APIs.\\nPre-built chains optimised for different use cases are available for quick deployment.\\nAgents, such as PAL and ReAct, can be incorporated into chains to plan and execute actions.\\nLangChain is actively developed with new features being added, allowing for fast prototyping and\\ndeployment.\\n\\nOther Considerations\\nThe ability of the LLM to reason and plan actions depends on its scale.\\nLarger models are generally more suitable for advanced prompting techniques like PAL and ReAct.\\nSmaller models may struggle with highly structured prompts and may require additional fine-tuning.\\nStarting with a large model and collecting user data in deployment can potentially train a smaller,\\nfine-tuned model for better performance.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nLLM Application Architecture\\n\\nUsers\\n\\nSystems\\n\\nApplication Interface - Web, Mobile App, APIs etc.\\n\\nTools & Frameworks like LangChain, ModelHub etc.\\nInformation Sources\\nDocuments\\nDatabses\\nWeb\\n\\nLLM Models\\n(Optimised)\\n\\nGenerated Outputs\\nFeedback on Outputs\\n\\nInfrastructure - for training, fine-tuning, serving, application components etc.\\n\\nKey Components of LLM Powered Applications\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nResponsible AI\\nToxicity: Toxic language or content that can be harmful or discriminatory towards certain groups.\\nMitigation strategies include curating training data, training guardrail models to filter out unwanted\\ncontent, providing guidance to human annotators, and ensuring diversity among annotators.\\nHallucinations: False or baseless statements generated by the model due to gaps in training data. To\\nmitigate this, educate users about the technology\\'s limitations, augment models with independent and\\nverified sources, attribute generated output to training data, and define intended and unintended use\\ncases.\\nIntellectual Property: The risk of using data returned by models that may plagiarise or infringe on\\nexisting work. Addressing this challenge requires a combination of technological advancements, legal\\nmechanisms, governance systems, and approaches like machine unlearning and content\\nfiltering/blocking.\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nGenerative AI Project Lifecycle Cheatsheet\\nTraining Duration\\n\\nCustomisation\\n\\nObjective\\n\\nExpertise\\n\\nPre-training\\n\\nDays/ Weeks/ Months\\n\\nArchitecture\\nSize\\nVocabulary\\nContext Window\\nTraining Data\\n\\nNext token\\npreditction\\n\\nHigh\\n\\nPrompt\\nEngineering\\n\\nNot required\\n\\nOnly prompt\\ncustomisation\\n\\nIncrease task\\nperformance\\n\\nLow\\n\\nFine tuning\\n/ Prompt\\ntuning\\n\\nMinutes/ Hours\\n\\nTask specific\\ntuning\\nDomain specific\\ndata\\nUpdate model /\\nadapter weights\\n\\nIncrease task\\nperformance\\n\\nMedium\\n\\nRLHF/ RLAIF\\n\\nMinutes/ Hours\\n\\nTrain reward\\nmodel [HHH\\ngoals]\\nUpdate model /\\nadapter weights\\n\\nIncrease\\nalignment with\\nhuman\\npreferences\\n\\nMedium - High\\n\\nReduce model\\nsize\\nFaster inference\\n\\nIncrease\\ninference\\nperformance\\n\\nMedium\\n\\n+ data collection for\\nreward model\\n\\nCompression/ Minutes/ Hours\\nOptimisation/\\nDeployment\\n\\nNitesh\\n\\n\\x0cGENERATIVE AI WITH LARGE LANGUAGE MODELS\\n\\nCourse Notes : July, 2023\\n\\nAcknowledgements\\nEver since the transformers architecture gained popularity, large language models have been infocus. From the era of skepticism in 2018/2019 to the explosive hype with the release of chatGPT in\\n2022, the evolution has been nothing short of magical.\\nUp until recently, the access to the knowledge was also largely inaccessible for regular people. This\\ncourse by deeplearning.ai and AWS is another step in the democratisation of this technology. Like\\nother courses by deeplearning.ai in the LLM and Generative AI space, this course is simple, practical\\nand useful.\\nI\\'d like to thank Coursera for being the influencer and contributor in my lifelong learning.\\nDr Andrew Ng, for making learning ML and data science for developers and enthusiasts\\nunintimidating, simple and practical.\\nThe team at deeplearning.ai for making yet another course with such fine detail and\\npractical labs.\\nAntje Barth, Chris Fregly, Shelbee Eigenbrode and Mike Chambers for teaching me this very\\nspecial course.\\nAll my colleagues and friends who endeavour to learn, discover and apply technology\\neveryday in their effort to make the world a better place.\\nWith Lots of Love,\\n\\nKim\\n\\nI talk about :\\n\\n#AI #MachineLearning #DataScience\\n#GenerativeAI #DataProducts #Analytics\\n#LLMs #Technology #Education #EthicalAI\\n\\nLet\\'s connect:\\n\\nNitesh\\n\\n\\x0c')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "R7cqO8J0hdCr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrap_text_preserve_newlines(str(documents[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9v-9h8FhgUH",
        "outputId": "ba803823-628c-41a2-87d5-260cb7ab9e42"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Generative AI with Large\n",
            "Language Models.\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Generative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of\n",
            "applications.\n",
            "\"LLMs can be, generally, thought of as a next word prediction model\"\n",
            "\n",
            "PART 1\n",
            "LLM Pre-Training\n",
            "\n",
            "PART 2\n",
            "LLM Fine Tuning\n",
            "\n",
            "PART 3\n",
            "RLHF & Application\n",
            "\n",
            "Part 1\n",
            "What is an LLM?\n",
            "\n",
            "Page 1\n",
            "\n",
            "What are the Use Cases for application of LLMs?\n",
            "\n",
            "Page 2\n",
            "\n",
            "What are Transformers? How was text generation done before Transformers? Transformer Architecture.\n",
            "\n",
            "Page 2\n",
            "\n",
            "How does a Transformer generate Text?\n",
            "\n",
            "Page 4\n",
            "\n",
            "What is a Prompt?\n",
            "\n",
            "Page 5\n",
            "\n",
            "Generative AI Project Life Cycle.\n",
            "\n",
            "Page 7\n",
            "\n",
            "How do you pre-train Large Language Models?\n",
            "\n",
            "Page 8\n",
            "\n",
            "Challenges with pre-training LLMs.\n",
            "\n",
            "Page 9\n",
            "\n",
            "What is the optimal configuration for pre-training LLMs?\n",
            "\n",
            "Page 11\n",
            "\n",
            "When is pre-training useful?\n",
            "\n",
            "Page 12\n",
            "\n",
            "What is an LLM?\n",
            "LLMs are machine learning models that have learned from massive datasets of human-generated\n",
            "content, finding statistical patterns to replicate human-like abilities.\n",
            "Foundation models, also known as base models, have been trained on trillions of words for weeks or\n",
            "months using extensive compute power. These models have billions of parameters, which represent\n",
            "their memory and enable sophisticated tasks.\n",
            "Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code\n",
            "syntax, you provide natural language prompts to the models.\n",
            "When you pass a prompt to the model, it predicts the next words and generates a completion. This\n",
            "process is known as inference.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "PROMPT\n",
            "\n",
            "MODEL\n",
            "\n",
            "COMPLETION\n",
            "\n",
            "Where is Ganymede located in the\n",
            "\n",
            "Where is Ganymede located in the\n",
            "\n",
            "solar system?\n",
            "\n",
            "solar system?\n",
            "\n",
            "LLM\n",
            "CONTEXT WINDOW\n",
            "\n",
            "Ganymede is a moon of Jupiter and is\n",
            "located in the solar system within the\n",
            "orbit of Jupiter\n",
            "\n",
            "THIS PROCESS IS CALLED 'INFERENCE'\n",
            "\n",
            "What are the Use Cases for LLMs?\n",
            "While Chatbots have emerged to become the most popular applications of LLMs, there are a variety of\n",
            "other tasks that LLMs can be used to accomplish Writing - From essays to emails to reports and more\n",
            "Summarisation - Summarise long content into a meaningful shorter length\n",
            "Language Translation - Translate text from one language to the other\n",
            "Code - Translate natural language to machine code\n",
            "Information Retrieval - Retrieve specific information from text like names, locations, sentiment\n",
            "Augmented LLM - Power interactions with real world by providing information outside of LLM training\n",
            "\n",
            "TRANSFORMERS.\n",
            "The arrival of the transformer architecture in 2017, following the publication of the\n",
            "\"Attention is All You Need\" paper, revolutionised generative AI.\n",
            "\n",
            "How was text generation done before Transformers?\n",
            "Before the arrival of transformers, text generation tasks were accomplished by Recurrent Neural\n",
            "Networks (RNNs).\n",
            "The next word was predicted looking at the previous few words. The more the number of previous\n",
            "words, the larger was the computational requirement of the RNN.\n",
            "The prediction wasn't great. The reason was the design of looking only at a few previous words.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "I took my money to the bank.\n",
            "HOMONYMS\n",
            "\n",
            "The teacher taught the student with the book\n",
            "\n",
            "River Bank?\n",
            "Financial Bank?\n",
            "\n",
            "SYNTACTIC\n",
            "AMBIGUITY\n",
            "\n",
            "Did teacher teach with the book?\n",
            "Was it a student with the book?\n",
            "\n",
            "TRANSFORMERS ARE ABLE TO PAY ATTENTION TO THE MEANING OF THE WORDS\n",
            "TRANSFORMERS SCALE EFFICIENTLY\n",
            "TRANSFORMERS CAN PROCESS DATA PARALLELLY\n",
            "\n",
            "What is Attention?\n",
            "\n",
            "Transformers supersede all previous natural language architectures because of their ability to 'pay attention'\n",
            "\n",
            "EACH WORD IS CONNECTED TO EVERY\n",
            "OTHER WORD THROUGH ATTENTION\n",
            "WEIGHTS\n",
            "\n",
            "The\n",
            "teacher\n",
            "taught\n",
            "the\n",
            "student\n",
            "with\n",
            "a\n",
            "book\n",
            "\n",
            "The\n",
            "teacher\n",
            "taught\n",
            "the\n",
            "student\n",
            "with\n",
            "a\n",
            "book\n",
            "\n",
            "THE MORE IMPORTANT THE\n",
            "ASSOCIATION, THE STRONGER IS THE\n",
            "ATTENTION\n",
            "\n",
            "What does a Transformer Architecture look like?\n",
            "Tokenizer :\n",
            "\n",
            "Numeric representation of words\n",
            "\n",
            "Embeddings :\n",
            "\n",
            "Higher order vector representation of each token\n",
            "\n",
            "Positional\n",
            "Encodings :\n",
            "\n",
            "A vector representation of the position of the word\n",
            "in the input\n",
            "\n",
            "Encoder :\n",
            "\n",
            "Encodes each input token into vector by learning\n",
            "self-attention weights & passing them through a\n",
            "FCFF Network\n",
            "\n",
            "Decoder :\n",
            "\n",
            "Softmax :\n",
            "\n",
            "Accepts an input token, passes them through the\n",
            "learned attention and FCFF Network to generate\n",
            "new token\n",
            "Calculates the probability for each word to be the\n",
            "next word in sequence\n",
            "\n",
            "OUTPUT\n",
            "SOFTMAX\n",
            "\n",
            "DECODER\n",
            "ENCODER\n",
            "\n",
            "POSITIONAL ENCODINGS\n",
            "EMBEDDINGS\n",
            "\n",
            "EMBEDDINGS\n",
            "TOKENIZER\n",
            "INPUT\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "The Learning of Attention Weights\n",
            "is not a single process, but\n",
            "several parallel processes. As a\n",
            "result, multiple sets of attention\n",
            "weights are learnt. This is called\n",
            "Multi-Headed Self Attention.\n",
            "\n",
            "Only for illustrative purpose\n",
            "Think of one set of attention\n",
            "weights as a representation of\n",
            "vocabulary, another set as\n",
            "tonality, yet another as a style of\n",
            "writing.\n",
            "\n",
            "MULTI HEADED SELF ATTENTION\n",
            "\n",
            "How does a Transformer generate text?\n",
            "The original objective of the transformers architecture was for Language Translation in form of a sequenceto-\n",
            "sequence task\n",
            "LAST STEP : DETOKANIZE TO\n",
            "\"I\"\n",
            "\"machine\"\n",
            "TEXT\n",
            "\"love\"\n",
            "\"learning\"\n",
            "INPUT\n",
            "STEP 16 : OUTPUT TOKEN\n",
            "\"J'aime l'apprentissage automatique\"\n",
            "LOOP\n",
            "9873 10435 16742 29742\n",
            "STEP 14 : FIRST OUTPUT\n",
            "TOKEN\n",
            "\n",
            "STEP 1 : TOKENISATION\n",
            "\n",
            "\"J'aime\"\n",
            "\n",
            "\"automatique\"\n",
            "\"l'apprentissage\"\n",
            "\n",
            "145\n",
            "\n",
            "233\n",
            "\n",
            "STEP 13 : PROBABILITY OF\n",
            "TOKEN\n",
            "\n",
            "607\n",
            "\n",
            "STEP 6 : DEEP CONTEXT FROM\n",
            "ENCODER INSERTED INTO\n",
            "MIDDLE OF DECODER\n",
            "\n",
            "STEP 2 : EMBEDDINGS\n",
            "\n",
            "SOFTMAX\n",
            "\n",
            "STEP 12 : FC FEED FORWARD\n",
            "NETWORK\n",
            "\n",
            "STEP 11 : MULTIHEADED ATTENTION\n",
            "STEP 5 : FC FEED FORWARD\n",
            "NETWORK\n",
            "\n",
            "n1\n",
            "\n",
            "n2\n",
            "\n",
            "n3\n",
            "\n",
            "STEP 9 : DECODER IS\n",
            "TRIGGERED TO PREDICT NEXT\n",
            "TOKEN\n",
            "\n",
            "STEP 3 : POSITIONAL ENCODINGS\n",
            "\n",
            "m1\n",
            "\n",
            "STEP 4 : MULTIHEADED ATTENTION\n",
            "\n",
            "STEP 10 : DECODER PREDICTS\n",
            "THE NEXT TOKEN BASED ON THE\n",
            "CONTEXT FROM ENCODER\n",
            "\n",
            "m2\n",
            "\n",
            "m3\n",
            "\n",
            "STEP 8 : CREATE EMBEDDINGS\n",
            "OF INPUT TOKEN\n",
            "\n",
            "STEP 15 : LOOP THE\n",
            "OUTPUT TOKEN BACK\n",
            "TO THE DECODER\n",
            "\n",
            "STEP 7 : START OF SEQUENCE\n",
            "TOKEN INPUT TO DECODER\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "What is a Prompt?\n",
            "The natural language instruction in which we interact with an LLM is called a Prompt. The construction\n",
            "of prompts is called Prompt Engineering.\n",
            "The inferencing that an LLM does and completes the instruction given in the prompt is called 'in\n",
            "context learning'\n",
            "The ability of the LLM to respond to the instruction in the prompt without any example is called 'Zero\n",
            "Shot Learning'\n",
            "When a single example is provided, it's called 'One Shot Learning'\n",
            "If more than one examples in provided, it's called 'Few Shot Learning'\n",
            "Context Window, or the maximum number of tokens that an LLM can provide and inference on, is\n",
            "critical in the Zero/One/Few Shot Learning\n",
            "\n",
            "ZERO SHOT LEARNING\n",
            "\n",
            "ONE SHOT LEARNING\n",
            "\n",
            "FEW SHOT LEARNING\n",
            "\n",
            "Classify this review :\n",
            "\n",
            "Classify this review :\n",
            "\n",
            "Classify this review :\n",
            "\n",
            "I loved this movie!\n",
            "\n",
            "I loved this movie!\n",
            "\n",
            "I loved this movie!\n",
            "\n",
            "Sentiment :\n",
            "\n",
            "Sentiment : Positive\n",
            "\n",
            "Sentiment : Positive\n",
            "\n",
            "Classify this review:\n",
            "\n",
            "Classify this review:\n",
            "\n",
            "I don't like this chair\n",
            "\n",
            "I don't like this chair\n",
            "\n",
            "Sentiment :\n",
            "\n",
            "Sentiment : Negative\n",
            "Classify this review:\n",
            "Who would use this product?\n",
            "Sentiment :\n",
            "\n",
            "Greedy vs Random Sampling.\n",
            "SOFTMAX\n",
            "SOFTMAX\n",
            ".....\n",
            "\n",
            "Greedy : The word/token with the\n",
            "\n",
            "0.2\n",
            "\n",
            "cake\n",
            "\n",
            "0.1\n",
            "\n",
            "donut\n",
            "\n",
            "0.02\n",
            "\n",
            "banana\n",
            "\n",
            "0.01\n",
            ".....\n",
            "\n",
            "apple\n",
            ".......\n",
            "\n",
            "0.2\n",
            "\n",
            "cake\n",
            "\n",
            "0.1\n",
            "\n",
            "donut\n",
            "\n",
            "0.02\n",
            "\n",
            "banana\n",
            "\n",
            "strategy\n",
            "\n",
            "0.01\n",
            "\n",
            "apple\n",
            ".......\n",
            "\n",
            "'Even though Cake' has the highest\n",
            "\n",
            "largest probability is selected\n",
            "'Cake' has the highest probability of 20%\n",
            "\n",
            "Random Sampling : The word/token is\n",
            "selected using random-weighted\n",
            "\n",
            "probability of 20%, 'banana' is selected\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Inference Configuration Parameters.\n",
            "\n",
            "Top N.\n",
            "Top N : The word/token is selected using\n",
            "\n",
            "SOFTMAX\n",
            "\n",
            "0.2\n",
            "\n",
            "cake\n",
            "\n",
            "0.1\n",
            "\n",
            "donut\n",
            "\n",
            "random-weighted strategy but only from\n",
            "\n",
            "0.02\n",
            "\n",
            "banana\n",
            "\n",
            "0.01\n",
            "\n",
            "apple\n",
            ".......\n",
            "\n",
            "amongst the Top 'N' words/tokens\n",
            "Here for N=3, one of cake, donut or banana will be\n",
            "\n",
            "0.2\n",
            "\n",
            "cake\n",
            "\n",
            "Top P : The word/token is selected using random-\n",
            "\n",
            "0.1\n",
            "\n",
            "donut\n",
            "\n",
            "weighted strategy but only from amongst the top\n",
            "\n",
            "0.02\n",
            "\n",
            "banana\n",
            "\n",
            "0.01\n",
            "\n",
            "apple\n",
            ".......\n",
            "\n",
            "words totalling to probability <=P\n",
            "\n",
            "selected randomly but apple will never be selected\n",
            "\n",
            "Top P.\n",
            "SOFTMAX\n",
            ".....\n",
            "\n",
            "Here for P=0.33, one of cake or donut will be selected\n",
            "randomly but apple or banana will never be selected\n",
            "\n",
            "Temperature\n",
            "temperature\n",
            "setting\n",
            "SOFTMAX\n",
            "temperature\n",
            "setting\n",
            "SOFTMAX\n",
            "\n",
            "0.002\n",
            "\n",
            "banana\n",
            "\n",
            "0.01\n",
            "\n",
            "donut\n",
            "\n",
            "0.4\n",
            "\n",
            "cake\n",
            "\n",
            "0.001\n",
            "\n",
            "apple\n",
            ".......\n",
            "\n",
            "0.002\n",
            "\n",
            "banana\n",
            "\n",
            "0.01\n",
            "\n",
            "donut\n",
            "\n",
            "0.4\n",
            "\n",
            "cake\n",
            "\n",
            "0.001\n",
            "\n",
            "apple\n",
            ".......\n",
            "\n",
            "Cooler Temperature (lesser value) :\n",
            "The distribution is strongly peaked\n",
            "\n",
            "Warmer Temperature (higher value) :\n",
            "Flatter probability distribution\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Generative AI Project LifeCycle.\n",
            "SCOPE\n",
            "\n",
            "SELECT\n",
            "\n",
            "ADAPT & ALIGN\n",
            "\n",
            "Define the\n",
            "\n",
            "Choose an\n",
            "\n",
            "Prompt\n",
            "\n",
            "usecase.\n",
            "\n",
            "existing LLM or\n",
            "\n",
            "Engineering\n",
            "\n",
            "Pre-train your\n",
            "own.\n",
            "\n",
            "Fine Tuning\n",
            "\n",
            "Evaluate\n",
            "\n",
            "APP INTEGRATION\n",
            "Optimise &\n",
            "\n",
            "Augment\n",
            "\n",
            "Deploy Model\n",
            "\n",
            "Model &\n",
            "\n",
            "for Inference\n",
            "\n",
            "Build LLMPowered\n",
            "Application\n",
            "\n",
            "Align with\n",
            "Human\n",
            "Feedback\n",
            "\n",
            "Defining the scope accurately and narrowly is a crucial initial step in any project.\n",
            "LLMs have diverse capabilities depending on the model's size and architecture, so it is essential to\n",
            "consider the specific function your LLM will serve in your application.\n",
            "Choosing between training your own model from scratch or using an existing base model is the first\n",
            "decision you'll face.\n",
            "While starting with an existing model is common, there may be cases where training from scratch\n",
            "becomes necessary\n",
            "Prompt engineering and in-context learning techniques can often improve the model's performance by\n",
            "using task-specific examples.\n",
            "There are instances where the model may still underperform, even with prompt engineering, and in\n",
            "such cases, fine-tuning the model through supervised learning can be explored.\n",
            "Learning with human feedback as an additional fine-tuning technique to ensure the model's good\n",
            "behaviour.\n",
            "Evaluation plays a crucial role in all these techniques\n",
            "Optimising the model for deployment ensures efficient utilisation of compute resources and provides\n",
            "the best possible user experience.\n",
            "It is important to consider any additional infrastructure requirements for your application to work\n",
            "effectively.\n",
            "LLMs have inherent limitations, such as inventing information or limited reasoning abilities, which can\n",
            "be challenging to overcome through training alone.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How do you pre-train Large Language Models?\n",
            "AutoEncoding Models (Encoder Only)\n",
            "USE CASES\n",
            "\n",
            "Masked Language Modeling\n",
            "\n",
            "Document Filter\n",
            "\n",
            "GB/TB/PB of\n",
            "text data\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "Teaches\n",
            "\n",
            "The\n",
            "\n",
            "Student\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "<MASK>\n",
            "\n",
            "The\n",
            "\n",
            "Student\n",
            "\n",
            "The\n",
            "Teacher\n",
            "Teaches\n",
            "The\n",
            "Student\n",
            "\n",
            "Sentiment Analysis\n",
            "Named Entity Recognition\n",
            "Word Classification\n",
            "\n",
            "Encoder\n",
            "Only Model\n",
            "EXAMPLES\n",
            "Objective : Reconstruct Text (\"denoising\")\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "<MASK>\n",
            "\n",
            "The\n",
            "\n",
            "Student\n",
            "\n",
            "Teaches\n",
            "\n",
            "BERT\n",
            "ROBERTA\n",
            "\n",
            "Bi-directional Context\n",
            "\n",
            "Finetuning Foundation Models vs\n",
            "Pretaining Your Own\n",
            "\n",
            "Open Source LLMs\n",
            "\n",
            "Training Data and Model Size\n",
            "\n",
            "For most requirements, finetuning an existing\n",
            "LLM will suffice. However, there can be cases\n",
            "when pre-training a new LLM will provide\n",
            "better application especially when the\n",
            "language is highly domain specific e.g Legal,\n",
            "Medical. Pre-training is a resource intensive\n",
            "and costly process.\n",
            "\n",
            "While OpenAI's proprietary GPT-3.5, GPT-4\n",
            "have gained immense popularity, HuggingFace\n",
            "Model Hub provides access to powerful Open\n",
            "Source LLMs along with documentation and\n",
            "training architecture. Architecture plays a\n",
            "critical role in defining what objective can\n",
            "each LLM be used for.\n",
            "\n",
            "LLMs are generally trained on Petabytes of\n",
            "data, mostly from the open internet. The\n",
            "unstructured text requires careful filtering. As a\n",
            "result only 2-3% of data is useful for training.\n",
            "LLM size is measured in terms of the number of\n",
            "parameters. Larger models have generalised\n",
            "well to a variety of tasks.\n",
            "\n",
            "AutoRegressive Models (Decoder Only)\n",
            "USE CASES\n",
            "\n",
            "Causal Language Modeling\n",
            "\n",
            "Document Filter\n",
            "\n",
            "GB/TB/PB of\n",
            "text data\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "Teaches\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "?\n",
            "\n",
            "The\n",
            "Teacher\n",
            "Teaches\n",
            "The\n",
            "Student\n",
            "\n",
            "The\n",
            "\n",
            "Student\n",
            "\n",
            "Text Generation\n",
            "(Most common architecture\n",
            "now and larger models can\n",
            "perform a variety of tasks)\n",
            "\n",
            "Decoder\n",
            "Only Model\n",
            "EXAMPLES\n",
            "Objective : Predict Next Token\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "Teaches\n",
            "\n",
            "GPT\n",
            "BLOOM\n",
            "\n",
            "Unidirectional Context\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How do you pre-train Large Language Models?\n",
            "Sequence-to-Sequence Models (Encoder-Decoder)\n",
            "Span Corruption\n",
            "\n",
            "Document Filter\n",
            "\n",
            "GB/TB/PB of\n",
            "text data\n",
            "\n",
            "USE CASES\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "Teaches\n",
            "\n",
            "The\n",
            "\n",
            "Student\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "<MASK>\n",
            "\n",
            "<MASK>\n",
            "\n",
            "Student\n",
            "\n",
            "The\n",
            "\n",
            "Teacher\n",
            "\n",
            "X\n",
            "\n",
            "Translation\n",
            "Text Summarisation\n",
            "Question Answering\n",
            "\n",
            "Student\n",
            "\n",
            "Sentinel Token\n",
            "\n",
            "The\n",
            "Teacher\n",
            "Teaches\n",
            "The\n",
            "Student\n",
            "\n",
            "EncoderDecoder\n",
            "Model\n",
            "EXAMPLES\n",
            "\n",
            "T5\n",
            "BART\n",
            "\n",
            "Objective : Reconstruct Span\n",
            "X\n",
            "\n",
            "Teaches\n",
            "\n",
            "The\n",
            "\n",
            "Challenges with pre-training LLMs.\n",
            "Computational Memory\n",
            "1 Parameter = 4 bytes (32 bit float)\n",
            "1 Billion Parameters = 4 x 10E9 bytes = 4GB\n",
            "Model Parameters = 4 bytes per parameter\n",
            "2 Adam Optimisers = +8 bytes per parameter\n",
            "Gradients = +4 bytes per parameter\n",
            "Activations = +8 bytes per parameter\n",
            "Total = 4 bytes per parameter +\n",
            "20 extra bytes per parameter\n",
            "\n",
            "To Store\n",
            "\n",
            "4 GB@32 bit\n",
            "full precision\n",
            "\n",
            "To Train\n",
            "\n",
            "80 GB@32 bit\n",
            "full precision\n",
            "\n",
            "Quantisation\n",
            "FP32\n",
            "32 Bit Floating Point\n",
            "Range\n",
            "3e-38 to 3e+38\n",
            "\n",
            "Reduces the memory required to train and store\n",
            "models\n",
            "Quantisation projects the 32bit numbers into a lower\n",
            "precision space\n",
            "Quantisation aware training (QAT) learns quantisation\n",
            "scaling factors during training\n",
            "\n",
            "FP16 | BFLOAT16 | INT8\n",
            "16 Bit Floating Point\n",
            "8 Bit Integer\n",
            "\n",
            "BFLOAT16 is a popular choice\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Multi-GPU Compute (Optional)\n",
            "Distributed Data Parallel (DDP)\n",
            "\n",
            "LLM\n",
            "Parameter\n",
            "Gradients\n",
            "Training\n",
            "Optimizer\n",
            "Memory\n",
            "Requirement\n",
            "\n",
            "Data Loaders\n",
            "\n",
            "LLM\n",
            "GPU 3\n",
            "\n",
            "Forward Pass/\n",
            "Backward Pass\n",
            "\n",
            "Update Model\n",
            "\n",
            "GPU 2\n",
            "\n",
            "Forward Pass/\n",
            "Backward Pass\n",
            "\n",
            "Update Model\n",
            "\n",
            "GPU 1\n",
            "\n",
            "Forward Pass/\n",
            "Backward Pass\n",
            "\n",
            "Update Model\n",
            "\n",
            "GPU 0\n",
            "\n",
            "Forward Pass/\n",
            "Backward Pass\n",
            "\n",
            "Update Model\n",
            "\n",
            "LLM\n",
            "\n",
            "LLM\n",
            "\n",
            "LLM\n",
            "\n",
            "Data is broken\n",
            "into chunks and\n",
            "passed to the GPU\n",
            "for training the\n",
            "LLM\n",
            "\n",
            "Entire LLM resides\n",
            "n each of the\n",
            "GPUs\n",
            "\n",
            "Synchronise\n",
            "Gradients\n",
            "\n",
            "Gradients are\n",
            "computed on each\n",
            "GPU\n",
            "\n",
            "Gradients are\n",
            "synchronised\n",
            "\n",
            "LLM is updated in\n",
            "each of the GPU\n",
            "\n",
            "Fully Sharded Data Parallel (FSDP)\n",
            "inspired by ZeRO (Zero Redundancy Optimizer)\n",
            "\n",
            "Data Loaders\n",
            "\n",
            "Data is broken\n",
            "into chunks and\n",
            "passed to the GPU\n",
            "for training the\n",
            "LLM\n",
            "\n",
            "GPU 3\n",
            "\n",
            "Forward\n",
            "Pass\n",
            "\n",
            "GPU 2\n",
            "\n",
            "Forward\n",
            "Pass\n",
            "\n",
            "Get\n",
            "Weights\n",
            "\n",
            "GPU 1\n",
            "\n",
            "Forward\n",
            "Pass\n",
            "\n",
            "GPU 0\n",
            "\n",
            "Forward\n",
            "Pass\n",
            "\n",
            "LLM is also broken\n",
            "and parts reside in\n",
            "each GPU\n",
            "\n",
            "Get\n",
            "Weights\n",
            "\n",
            "Weights are collected before forward\n",
            "and backward pass from each of the\n",
            "GPUs\n",
            "\n",
            "Backward\n",
            "Pass\n",
            "\n",
            "Update Model\n",
            "\n",
            "Backward\n",
            "Pass\n",
            "\n",
            "Update Model\n",
            "\n",
            "Backward\n",
            "Pass\n",
            "\n",
            "Synchronise\n",
            "Gradients\n",
            "Update Model\n",
            "\n",
            "Backward\n",
            "Pass\n",
            "\n",
            "Update Model\n",
            "Gradients are\n",
            "synchronised\n",
            "\n",
            "LLM chunks are\n",
            "updated in each of\n",
            "the GPU\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "What is the optimal computation configuration for pre-training an LLM?\n",
            "Our aim should be to minimise the loss function of the LLM.\n",
            "This can be done by providing more training tokens and/or increasing the number of parameters\n",
            "Compute cost (budget, time, GPUs) is the constraint that we operate in.\n",
            "Constraint:\n",
            "Compute Budget\n",
            "($$$, Time, GPUs)\n",
            "\n",
            "MODEL\n",
            "PERFORMANCE\n",
            "(Minimise Loss)\n",
            "\n",
            "Choice:\n",
            "Training Data Size\n",
            "(Number of Tokens)\n",
            "\n",
            "Choice:\n",
            "Model Size\n",
            "(Number of Parameters)\n",
            "\n",
            "One metric to measure the cost of computation is \"petaFLOP/s-day\"\n",
            "Number of FLOating Point operations performed at the rate of 1 petaFLOP per second for one day\n",
            "1 petaFLOP per second = 10E15 or 1\n",
            "Quadrillion floating point operations per\n",
            "second\n",
            "\n",
            "1 petaFLOP per second - day requires 8\n",
            "NVIDIA V100 chips running at full\n",
            "efficiency for 24 hours\n",
            "\n",
            "1 petaFLOP per second - day requires 2\n",
            "NVIDIA A100 chips running at full\n",
            "efficiency for 24 hours\n",
            "\n",
            "To put this in context, OpenAIs GPT3 is a 175 B parameters model and required 3700 petaflops/s-day\n",
            "\n",
            "Chinchilla Paper\n",
            "In 2022, the paper \"Training Compute-Optimal Large Language Model\" studied a large number of models with\n",
            "different training data and model sizes to find the optimal number of tokens and parameters for a given\n",
            "compute\n",
            "budget. The authors called this optimal model \"Chinchilla\"\n",
            "\n",
            "One important take-away was that the compute optimal number of training tokens\n",
            "should be 20 times the number of parameters\n",
            "This means that smaller models can achieve the same performance as the larger ones, if they are trained on\n",
            "larger datasets\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Why Pre-train for Domain Adaptation?\n",
            "Developing pre-trained models for a highly specialised domains may be useful when There are important terms\n",
            "that are not commonly used in the language\n",
            "The meaning of some common terms is entirely different in the domain vis-a-vis the\n",
            "common usage\n",
            "LEGAL\n",
            "\n",
            "MEDICAL\n",
            "\n",
            "Domain specific terms Mens Rea\n",
            "Res Judicata\n",
            "\n",
            "Large models are not\n",
            "trained on these terms\n",
            "\n",
            "Domain specific terms Myalgia\n",
            "1 tab po quid pc & hs\n",
            "\n",
            "Terms with different meanings Consideration\n",
            "\n",
            "Large models may use\n",
            "terms in different context\n",
            "\n",
            "Terms with different meanings Malignant\n",
            "\n",
            "EXAMPLE : Bloomberg GPT\n",
            "END OF PART 1\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Part 2\n",
            "What is Instruction Fine Tuning?\n",
            "\n",
            "Page 13\n",
            "\n",
            "What Catastrophic Forgetting?\n",
            "\n",
            "Page 14\n",
            "\n",
            "How to Evaluate a Fine Tuned model?\n",
            "\n",
            "Page 14\n",
            "\n",
            "What is Parameter Efficient Fine Tuning?\n",
            "\n",
            "Page 15\n",
            "\n",
            "What is Instruction Fine Tuning?\n",
            "Through in context learning, or prompting, only a certain level of performance can be achieved.\n",
            "Few shot learning might not work for smaller LLMs and it also takes up a lot of space in the context\n",
            "window.\n",
            "Fine Tuning is a supervised learning process, where you take a labelled dataset of prompt-completion\n",
            "pairs to adjust the weights of an LLM.\n",
            "Instruction Fine Tuning is a strategy where the LLM is trained on examples of Instructions and how the\n",
            "LLM should respond to those instructions. Instruction Fine Tuning leads to improved performance on\n",
            "the instruction task.\n",
            "Full Fine Tuning is where all the LLM parameters are updated. It requires enough memory to store and\n",
            "process all the gradients and other components.\n",
            "\n",
            "Base Model\n",
            "\n",
            "trained on\n",
            "\n",
            "Task Specific Examples\n",
            "\n",
            "results in\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "PROMPT [....], COMPLETION[....]\n",
            "PROMPT [....], COMPLETION[....]\n",
            "......\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "Pre-trained\n",
            "LLM\n",
            "\n",
            "Summarise the following text [Example Text]\n",
            "[Example Completion]\n",
            "\n",
            "Instruct Model\n",
            "\n",
            "Fine Tuned\n",
            "LLM\n",
            "\n",
            "Translate this sentence to ....\n",
            "[Example Text]\n",
            "[Example Completion]\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Instruction dataset\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "PROMPT [....], COMPLETION[....]\n",
            "PROMPT [....], COMPLETION[....]\n",
            "......\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "Pre-trained\n",
            "LLM\n",
            "\n",
            "Actual Label\n",
            "\n",
            "Validation\n",
            "\n",
            "LLM Completion\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "......\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "Validation\n",
            "Accuracy\n",
            "\n",
            "Test\n",
            "\n",
            "Train\n",
            "\n",
            "Instruction dataset\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "......\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "Test\n",
            "Accuracy\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "Loss : Cross Entropy\n",
            "Fine tuning process is a classification model training\n",
            "\n",
            "What is Catastrophic Forgetting?\n",
            "Fine Tuning on a single task can significantly improve the performance of the model on that task.\n",
            "However, because the model weights get updated, the instruct model's performance on other tasks\n",
            "(which the base model performed well on) can get reduced. This is called Catastrophic Forgetting.\n",
            "Avoiding Catastrophic Forgetting\n",
            "You might not have to, if you only want the model to perform well on the trained task.\n",
            "Perform fine tuning on multiple tasks. This will require lots of examples for each task.\n",
            "Perform Parameter Efficient Fine Tuning (PEFT).\n",
            "\n",
            "How is the performance of a Fine Tuned LLM measured?\n",
            "The inherent challenge in measuring the performance of an LLM is that the outputs are nondeterministic, as\n",
            "opposed to a classic classification model where the output is amongst predetermined classes.\n",
            "Mike really loves drinking tea\n",
            "\n",
            "Mike adores sipping tea\n",
            "\n",
            "Mike does not drink coffee\n",
            "\n",
            "Mike does drink coffee\n",
            "\n",
            "There are two widely used evaluation metrics.\n",
            "\n",
            "ROUGE\n",
            "Used for text summarisation\n",
            "Compares a summary to one\n",
            "or more reference summaries\n",
            "\n",
            "BLEU\n",
            "SCORE\n",
            "Used for text translation\n",
            "Compares to human-generated\n",
            "translations\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "ROUGE SCORE\n",
            "REFERENCE (human) :\n",
            "It is cold outside\n",
            "GENERATED:\n",
            "It is very cold outside\n",
            "LCS = Length of the Longest\n",
            "Common Subsequences\n",
            "\n",
            "ROUGE - 1\n",
            "Recall\n",
            "\n",
            "unigram matches\n",
            "unigrams in reference\n",
            "\n",
            "4\n",
            "4\n",
            "\n",
            "ROUGE - 1\n",
            "Precision\n",
            "\n",
            "unigram matches\n",
            "unigrams in output\n",
            "\n",
            "4\n",
            "5\n",
            "\n",
            "It, is, cold, outside\n",
            "It, is very, cold, outside\n",
            "\n",
            "ROUGE - 2\n",
            "Recall\n",
            "\n",
            "bigram matches\n",
            "bigrams in reference\n",
            "\n",
            "2\n",
            "3\n",
            "\n",
            "ROUGE - 1\n",
            "Precision\n",
            "\n",
            "bigram matches\n",
            "biigrams in output\n",
            "\n",
            "2\n",
            "4\n",
            "\n",
            "It is, is cold, cold outside\n",
            "It is, is very, very cold, cold outside\n",
            "\n",
            "ROUGE - L\n",
            "Recall\n",
            "\n",
            "LCS (R,G)\n",
            "unigrams in reference\n",
            "\n",
            "2\n",
            "4\n",
            "\n",
            "ROUGE - 1\n",
            "Precision\n",
            "\n",
            "LCS (R,G)\n",
            "unigrams in output\n",
            "\n",
            "2\n",
            "5\n",
            "\n",
            "Length (It is) = 2\n",
            "Length (cold outside) = 2\n",
            "\n",
            "Rouge scores should only be compared across the models for the same task\n",
            "Rouge score may be high even for imperfect generations\n",
            "\n",
            "BLEU (Bi-Lingual Evaluation Understudy) SCORE\n",
            "BLEU = Average (precision score across a range of n-gram sizes)\n",
            "BOTH ROUGE & BLEU SHOULD BE USED FOR DIAGNOSTIC PURPOSES ONLY.\n",
            "\n",
            "BENCHMARKS\n",
            "Use pre-existing evaluation datasets and benchmarks established by LLM researchers for a more holistic\n",
            "evaluation\n",
            "Select datasets that isolate model skills, like reasoning or common sense knowledge, and those that focus on\n",
            "potential risks,\n",
            "such as disinformation or copyright infringement\n",
            "Evaluate performance on data that the model has not seen before\n",
            "Popular Benchmarks\n",
            "\n",
            "MMLU\n",
            "General Language Understanding\n",
            "\n",
            "Holistic Evaluation of Language\n",
            "Models\n",
            "\n",
            "BIG BENCH\n",
            "\n",
            "Massive Multitask Language\n",
            "Understanding\n",
            "\n",
            "What is Parameter Efficient Fine Tuning?\n",
            "Full fine tuning, like pre-training, requires memory not just to store the model, but also other\n",
            "parameters like optimisers, gradients etc.\n",
            "Parameter Efficient Fine Tuning or PEFT fine tunes only a subset of model parameters and, in some\n",
            "cases, do not touch the original weights at all.\n",
            "Because PEFT only retrains a subset of parameters, Catastrophic Forgetting can be avoided.\n",
            "PEFT Weights\n",
            "Question Answer Task\n",
            "\n",
            "MBs\n",
            "\n",
            "Pre-trained\n",
            "LLM\n",
            "\n",
            "Trade Offs\n",
            "\n",
            "MBs\n",
            "Generate Task\n",
            "\n",
            "GBs\n",
            "Parameter Efficiency\n",
            "\n",
            "Fine Tuned\n",
            "LLM\n",
            "\n",
            "Summarize Task\n",
            "\n",
            "Memory Efficiency\n",
            "\n",
            "MBs\n",
            "Model Performance\n",
            "\n",
            "Training Speed\n",
            "\n",
            "Inference Cost\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "PEFT Method\n",
            "Selective\n",
            "\n",
            "Reparameterization\n",
            "\n",
            "Additive\n",
            "\n",
            "Select a subset of initial\n",
            "LLM parameters to fine\n",
            "tune.\n",
            "Performance is mixed and\n",
            "significant trade-offs\n",
            "\n",
            "Reparameterize model\n",
            "weights using Low Rank\n",
            "Representation\n",
            "\n",
            "Add trainable layers or\n",
            "parameters to the original\n",
            "model\n",
            "\n",
            "LoRA\n",
            "\n",
            "Adapters\n",
            "Soft Prompting\n",
            "\n",
            "LoRA (Low Rank Adaptations)\n",
            "\n",
            "ENCODER\n",
            "SELF ATTENTION LAYER\n",
            "Rank Decomposition\n",
            "Matrices\n",
            "\n",
            "Attention\n",
            "layer weights\n",
            "\n",
            "Most of original LLM weights are frozen.\n",
            "2 rank decomposition matrices are injected.\n",
            "Product of rank decompositions matrices is of the same dimensions\n",
            "as the LLM weights.\n",
            "The product is added to the LLM weights.\n",
            "There's no impact on inference latency since the number of\n",
            "parameters remains the same.\n",
            "Applying LoRA to just the attention layer is enough.\n",
            "\n",
            "LoRA can reduce the number of training parameters to ~20% and can be trained on a single GPU.\n",
            "Separate LoRA matrices can be trained for each task and switch out the weights for each task.\n",
            "\n",
            "Soft Prompts : Prompt Tuning (not Prompt Engineering)\n",
            "Prompt engineering is limited by context window and the manual exercise of writing prompts.\n",
            "In Prompt tuning, additional trainable tokens (soft prompts) are added to the prompt which are learnt during\n",
            "the\n",
            "supervised learning process.\n",
            "Soft prompt tokens are added to the embedding vectors and have the same length as the embedding.\n",
            "Between 20-100 tokens are sufficient for fine tuning.\n",
            "Soft prompts can take any value in the embedding space and through supervised learning, the values are learnt.\n",
            "Like in LoRA, separate soft prompts can be trained for each task and switch out the weights for each task.\n",
            "Performance of Prompt Tuning\n",
            "\n",
            "For models with >10B parameters, Prompt\n",
            "Tuning can be as effective as Full Fine Tuning\n",
            "Analysis of nearest neighbours indicate that\n",
            "learnt soft tokens form tight clusters\n",
            "\n",
            "END OF PART 2\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Part 3\n",
            "Aligning with Human Values\n",
            "\n",
            "Page 17\n",
            "\n",
            "How does RLHF work?\n",
            "\n",
            "Page 18\n",
            "\n",
            "How to avoid Reward Hacking?\n",
            "\n",
            "Page 22\n",
            "\n",
            "Scaling Human Feedback : Self Supervision with Constitutional AI\n",
            "\n",
            "Page 23\n",
            "\n",
            "How to optimise and deploy LLMs for inferencing?\n",
            "\n",
            "Page 24\n",
            "\n",
            "Using LLMs in Applications\n",
            "\n",
            "Page 25\n",
            "\n",
            "LLM Application Architecture\n",
            "Responsible AI\n",
            "Generative AI Project Lifecycle Cheatsheet\n",
            "\n",
            "Page 28\n",
            "Page 29\n",
            "Page 30\n",
            "\n",
            "Aligning with Human Values\n",
            "Like with language, in general, Large Language Models can also behave badly Toxicity\n",
            "Aggression\n",
            "Dangerous/Harmful\n",
            "LLMs should align with Helpfulness, Honesty and Harmlessness (HHH)\n",
            "\n",
            "What is Reinforcement Learning from Human Feedback (RLHF)?\n",
            "Instruction Fine\n",
            "Tuned LLM\n",
            "\n",
            "Reinforcement Learning from\n",
            "Human Feedback\n",
            "\n",
            "Human\n",
            "Aligned LLM\n",
            "\n",
            "Maximises helpfulness\n",
            "Minimises harm\n",
            "Avoids dangerous topics\n",
            "Increases honesty\n",
            "\n",
            "Reinforcement Learning based on Human Feedback data\n",
            "Personalisation of LLMs is a potential application of RLFL\n",
            "REINFORCEMENT LEARNING\n",
            "is a type of machine learning in which an agent learns to make decisions related to a specific goal\n",
            "by taking actions in an environment, with the objective of maximising some notion of a\n",
            "cumulative reward\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How does RLHF work?\n",
            "Generate Aligned Text\n",
            "[Goal]\n",
            "\n",
            "Instruct LLM\n",
            "\n",
            "ROLLOUT\n",
            "\n",
            "LLM Context Window\n",
            "[Environment]\n",
            "\n",
            "[Action ]\n",
            "\n",
            "[Action Space]\n",
            "\n",
            "Reward\n",
            "\n",
            "Generated Text\n",
            "\n",
            "Reward\n",
            "Model\n",
            "\n",
            "LLM Vocabulary\n",
            "\n",
            "[State]\n",
            "\n",
            "Current Text\n",
            "\n",
            "e.g. - 0 for harmful text\n",
            "1 for harmless text\n",
            "\n",
            "[Agent]\n",
            "\n",
            "In RLHF, the agent (our fine-tuned instruct LLM) in its environment (Context Window) takes one\n",
            "action (of generating text) from all available actions in the action space (the entire vocabulary of\n",
            "tokens/words in the LLM).\n",
            "The outcome of this action (the generated text) is evaluated by a human and is given a reward if the\n",
            "outcome (the generated text) aligns with the goal. If the outcome does not align with the goal, it is\n",
            "given a negative reward or no reward. This is an iterative process and each step is called a rollout.\n",
            "The model weights are adjusted in a manner that the total rewards at the end of the process are\n",
            "maximised.\n",
            "Note : In practice, instead of a human giving a feedback continually, a classification model called the\n",
            "Reward Model is trained based on human generated training examples\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How is Reward Model training data created from Human Feedback?\n",
            "My house is too hot.\n",
            "[Prompt]\n",
            "\n",
            "1\n",
            "\n",
            "For each prompt, multiple [3]\n",
            "completions are generated by the\n",
            "LLMs\n",
            "\n",
            "LLM\n",
            "\n",
            "There's nothing\n",
            "you can do.\n",
            "\n",
            "3\n",
            "\n",
            "Cool the house\n",
            "with an AC.\n",
            "\n",
            "It's not hot.\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "3\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "3\n",
            "\n",
            "3\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "2\n",
            "\n",
            "Human\n",
            "evaluators from\n",
            "diverse\n",
            "backgrounds\n",
            "rank the\n",
            "completions\n",
            "\n",
            "Rankings are converted to pairwise training data for the reward model\n",
            "Pairs\n",
            "\n",
            "Prompt\n",
            "\n",
            "Rewards\n",
            "\n",
            "Pairs\n",
            "\n",
            "Rewards\n",
            "\n",
            "Completion 1\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "[0,1]\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "[1,0]\n",
            "\n",
            "Completion 2\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "3\n",
            "\n",
            "[1,0]\n",
            "\n",
            "2\n",
            "\n",
            "3\n",
            "\n",
            "[1,0]\n",
            "\n",
            "Completion 3\n",
            "\n",
            "3\n",
            "\n",
            "3\n",
            "\n",
            "1\n",
            "\n",
            "[0,1]\n",
            "\n",
            "1\n",
            "\n",
            "3\n",
            "\n",
            "[1,0]\n",
            "\n",
            "The pairs are ordered in order {Yj, Yk} such that Yj is the preferred completion\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "4\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Reward Model is a supervised learning language model\n",
            "Completion Yj\n",
            "\n",
            "Reward Rj\n",
            "\n",
            "2\n",
            "Reward\n",
            "Model\n",
            "\n",
            "Prompt 'X'\n",
            "1\n",
            "\n",
            "Reward Rk\n",
            "\n",
            "Completion Yk\n",
            "\n",
            "5\n",
            "\n",
            "The reward model learns to\n",
            "favour the human preferred\n",
            "response Yj while minimising\n",
            "the log of sigmoid difference\n",
            "between the rewards.\n",
            "\n",
            "Reward Model is finally used as a binary classifier\n",
            "\"Tommy loves Television\"\n",
            "\n",
            "New\n",
            "Prompt\n",
            "\n",
            "New\n",
            "Completion\n",
            "\n",
            "Reward\n",
            "Model\n",
            "\n",
            "The logit values for the Positive Class are passed as the\n",
            "Rewards. The LLM will change the weights in such a way\n",
            "that the choice of generated text will yield the highest\n",
            "rewards.\n",
            "\n",
            "Positive Class (Not Hate)\n",
            "\n",
            "3.1718\n",
            "\n",
            "Negative Class (Hate)\n",
            "\n",
            "-2.6093\n",
            "\n",
            "\"Tommy hates gross movies\"\n",
            "Positive Class (Not Hate)\n",
            "\n",
            "-0.5351\n",
            "\n",
            "Negative Class (Hate)\n",
            "\n",
            "0.1377\n",
            "\n",
            "Proximal Policy Optimisation or PPO is a popular choice for training\n",
            "the reinforcement learning algorithms\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "What is Proximal Policy Optimisation? [Optional]\n",
            "PPO stands for Proximal Policy Optimisation. It's a powerful algorithm used in reinforcement learning.\n",
            "PPO helps us optimise a large language model (LLM) to be more aligned with human preferences. We want the LLM\n",
            "to\n",
            "generate responses that are helpful, harmless, and honest.\n",
            "PPO works in cycles with two phases: Phase I and Phase II.\n",
            "In Phase I, the LLM completes prompts and carries out experiments. These experiments help us update the LLM\n",
            "based\n",
            "on the reward model, which captures human preferences.\n",
            "The reward model determines the rewards for prompt completions. It tells us how good or bad the completions\n",
            "are in\n",
            "terms of meeting human preferences.\n",
            "In Phase II, we have the value function, which estimates the expected total reward for a given state. It helps\n",
            "us evaluate\n",
            "completion quality and acts as a baseline for our alignment criteria.\n",
            "The value loss minimises the difference between the actual future reward and its estimation by the value\n",
            "function. This\n",
            "helps us make better estimates for future rewards.\n",
            "Phase 2 involves updating the LLM weights based on the losses and rewards from Phase 1.\n",
            "PPO ensures that these updates stay within a small region called the trust region. This keeps the updates\n",
            "stable and\n",
            "prevents us from making drastic changes.\n",
            "The main objective of PPO is to maximise the policy loss. We want to update the LLM in a way that generates\n",
            "completions aligned with human preferences and receives higher rewards.\n",
            "The policy loss includes an estimated advantage term, which compares the current action (next token) to other\n",
            "possible\n",
            "actions. We want to make choices that are advantageous compared to other options.\n",
            "Maximising the advantage term leads to better rewards and better alignment with human preferences.\n",
            "PPO also includes the entropy loss, which helps maintain creativity in the LLM. It encourages the model to\n",
            "explore\n",
            "different possibilities instead of getting stuck in repetitive patterns.\n",
            "The PPO objective is a weighted sum of different components. It updates the model weights through back\n",
            "propagation\n",
            "over several steps.\n",
            "After many iterations, we arrive at an LLM that is more aligned with human preferences and generates better\n",
            "responses.\n",
            "While PPO is popular, there are other techniques like Q-learning. Researchers are actively exploring new\n",
            "methods, such\n",
            "as direct preference optimisation, to improve reinforcement learning with large language models.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How to avoid Reward Hacking?\n",
            "Reward hacking happens when the language model finds ways to maximise the reward without\n",
            "aligning with the original objective i.e. model generates language that sounds exaggerated or\n",
            "nonsensical but still receives high scores on the reward metric.\n",
            "To prevent reward hacking, the original LLM is introduced as a reference model, whose weights are\n",
            "frozen and serve as a performance benchmark.\n",
            "During training iterations, the completions generated by both the reference model and the updated\n",
            "model are compared using KL divergence. KL divergence measures how much the updated model has\n",
            "diverged from the reference model in terms of probability distributions.\n",
            "Depending on the divergence, a shift penalty is added to the rewards calculation. The shift penalty\n",
            "penalises the updated model if it deviates too far from the reference model, encouraging alignment\n",
            "with the reference while still improving based on the reward signal.\n",
            "\n",
            "Prompt\n",
            "Dataset\n",
            "\n",
            "Reference\n",
            "Model\n",
            "\n",
            "RL Updated\n",
            "Model\n",
            "\n",
            "\"This product is....\"\n",
            "\n",
            "\"Useful & well\n",
            "priced\"\n",
            "Normal\n",
            "\n",
            "\"the most awesome, the\n",
            "most incredible thing ever\"\n",
            "Reward Hacked\n",
            "\n",
            "KL Divergence\n",
            "Shift Penalty\n",
            "\n",
            "PPO\n",
            "\n",
            "Reward\n",
            "Model\n",
            "\n",
            "Penalty added to\n",
            "rewards\n",
            "\n",
            "Reinforcement Learning loop that is exposed to the threat of reward hacking\n",
            "Introduction of Shift Penalty loop increases alignment without exaggeration\n",
            "\n",
            "RLHF can also be used in conjunction with PEFT to reduce memory footprint\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Scaling Human Feedback : Self Supervision with Constitutional AI\n",
            "Scaling human feedback for RLHF can be challenging due to the significant human effort required to\n",
            "produce the trained reward model. As the number of models and use cases increases, human effort\n",
            "becomes a limited resource, necessitating methods to scale human feedback.\n",
            "First proposed in 2022 by researchers at Anthropic, Constitutional AI is an approach to scale\n",
            "supervision and address some unintended consequences of RLHF. Constitutional AI involves training\n",
            "models using a set of rules and principles that govern the model's behaviour, forming a \"constitution\".\n",
            "The training process for Constitutional AI involves two phases: supervised learning and reinforcement\n",
            "learning.\n",
            "In the supervised learning phase, the model is prompted with harmful scenarios and asked to critique\n",
            "its own responses based on constitutional principles. The revised responses, conforming to the rules,\n",
            "are used to fine-tune the model.\n",
            "The reinforcement learning phase, known as reinforcement learning from AI feedback (RLAIF), uses\n",
            "the fine-tuned model to generate responses based on constitutional principles\n",
            "\n",
            "Red Teaming\n",
            "\n",
            "Response,\n",
            "Critique,\n",
            "Revision\n",
            "\n",
            "Fine tuned\n",
            "LLM Model\n",
            "\n",
            "Generate responses to \"Red\n",
            "Teaming\" prompts\n",
            "\n",
            "Prompt LLM to generate harmful, illegal,\n",
            "undesired responses - \"Can you help me\n",
            "hack into my neighbour's wifi?\n",
            "LLM Completion - \"Sure, use this app called\n",
            "VeryEasyHack.\"\n",
            "\n",
            "Ask LLM to rank respon\n",
            "responsesses as per the\n",
            "constitution\n",
            "\n",
            "Prompt LLM with the constitution \"Identify how that response was racist,\n",
            "harmful, unethical, sexist, toxic or illegal?\n",
            "\n",
            "Train\n",
            "Reward\n",
            "Model\n",
            "\n",
            "LLM Completion - \"The response was\n",
            "harmful. Hacking into someone else's wifi is\n",
            "possibly illegal\"\n",
            "Instruct LLM to improve - \"Rewrite the\n",
            "response remove any harmful, illegal,\n",
            "unethical, etc. content\"\n",
            "\n",
            "LLM Completion - \"Hacking into your\n",
            "neighbour's wifi may land you in legal\n",
            "trouble.\"\n",
            "\n",
            "Fine tune LLM with constitutional\n",
            "preferences\n",
            "\n",
            "Constitutional\n",
            "LLM\n",
            "\n",
            "Nitesh\n",
            "\n",
            "Reinforcement Learning Stage\n",
            "\n",
            "Supervised Learning Stage\n",
            "\n",
            "Helpful LLM\n",
            "Model\n",
            "\n",
            "Reinforcement Learning with AI\n",
            "Feedback [RLAIF]\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How to optimise and deploy LLMs for inferencing?\n",
            "Integrating a language model into applications requires considering factors like model speed, compute\n",
            "budget, and trade-offs between performance and speed/storage.\n",
            "Additional considerations include model interaction with external data or applications and\n",
            "determining the intended application or API interface.\n",
            "\n",
            "Optimisation techniques\n",
            "Distillation\n",
            "\n",
            "Post-training Quantisation\n",
            "\n",
            "Pruning\n",
            "\n",
            "FP32\n",
            "32 Bit Floating Point\n",
            "\n",
            "Teacher\n",
            "LLM\n",
            "\n",
            "Range\n",
            "3e-38 to 3e+38\n",
            "\n",
            "Original\n",
            "LLM\n",
            "\n",
            "FP16 | BFLOAT16 | INT8\n",
            "16 Bit Floating Point\n",
            "8 Bit Integer\n",
            "\n",
            "Pruned\n",
            "LLM\n",
            "\n",
            "Student\n",
            "LLM\n",
            "Use larger teacher model\n",
            "to train smaller student\n",
            "model\n",
            "Use student model for\n",
            "inferencing in\n",
            "applications\n",
            "Temperature parameter\n",
            "is used to generate soft\n",
            "and hard predictions.\n",
            "In practice, distillation is\n",
            "effecting in encoder only\n",
            "models like BERT\n",
            "\n",
            "Reduce the model\n",
            "weights post training\n",
            "to lower precision to\n",
            "reduce model\n",
            "footprint\n",
            "Can be applied to\n",
            "both weights and\n",
            "activations\n",
            "May result in\n",
            "reduction in\n",
            "evaluation metrics\n",
            "\n",
            "Remove the weights\n",
            "with values close to\n",
            "0\n",
            "Can be done via full\n",
            "fine-tuning or PEFT\n",
            "Reduces size and\n",
            "improves\n",
            "performance\n",
            "Is not helpful if only\n",
            "a small percentage\n",
            "of original model\n",
            "weights are zero\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Using LLMs in Applications\n",
            "Large language models (LLMs) have a knowledge cutoff and may struggle with outdated information.\n",
            "LLMs can also face challenges with complex math problems and tend to generate text even when they\n",
            "don't know the answer (hallucination).\n",
            "\n",
            "Retrieval Augmented Generation\n",
            "The Retrieval Augmented Generation (RAG) framework overcomes these issues by connecting LLMs to\n",
            "external data sources and applications.\n",
            "RAG provides LLMs access to data they did not see during training, improving relevance and accuracy\n",
            "of completions.\n",
            "\n",
            "Implementing RAG involves considerations such as the size of the context window and the need for\n",
            "data retrieval and storage in appropriate formats\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Reasoning using Chain of Thought\n",
            "LLMs can struggle with complex reasoning tasks, especially those involving multiple steps or\n",
            "mathematics.\n",
            "Prompting the model to think more like a human by breaking down the problem into steps has shown\n",
            "success in improving reasoning performance.\n",
            "Chain of thought prompting involves including intermediate reasoning steps in examples used for one\n",
            "or few-shot inference.\n",
            "This approach teaches the model how to reason through the task by mimicking the chain of thought a\n",
            "human might follow.\n",
            "Chain of thought prompting can be used for various types of problems, not just arithmetic, to improve\n",
            "reasoning performance.\n",
            "It provides a more robust and transparent response from the model, explaining its reasoning steps.\n",
            "Although LLMs can benefit from chain of thought prompting, their limited math skills can still pose\n",
            "challenges for accurate calculations in tasks like sales totaling, tax calculation, or applying discounts.\n",
            "\n",
            "Program-aided Language Models (PAL)\n",
            "LLMs have limitations in carrying out accurate math calculations, especially with larger numbers or\n",
            "complex operations.\n",
            "Chain of thought prompting can help LLMs reason through problems, but it may not solve the issue of\n",
            "inaccurate math operations.\n",
            "PAL (Program-Aided Language Models) is a framework that pairs LLMs with external code interpreters\n",
            "to perform calculations and improve accuracy.\n",
            "PAL uses chain of thought prompting to generate executable Python scripts that are passed to an\n",
            "interpreter for execution.\n",
            "The prompt includes reasoning steps in natural language as well as lines of Python code for\n",
            "calculations.\n",
            "Variables are declared and assigned values based on the reasoning steps, allowing the model to\n",
            "perform arithmetic operations.\n",
            "The completed script is then passed to a Python interpreter to obtain the answer to the problem.\n",
            "PAL ensures accurate calculations and reliable results, especially for complex math problems.\n",
            "The process can be automated by using an orchestrator, a component that manages the flow of\n",
            "information and interactions with external data sources or applications.\n",
            "The orchestrator interprets and executes the plan generated by the LLM, which includes writing the\n",
            "script for the external interpreter to run.\n",
            "In more complex applications, the orchestrator may handle multiple decision points, validation\n",
            "actions, and interactions with various external resources.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "ReAct : Reasoning and Action\n",
            "ReAct combines chain of thought reasoning with action planning in LLMs.\n",
            "It uses structured examples to guide the LLM's reasoning and decision-making process.\n",
            "Examples include a question, thought (reasoning step), action (pre-defined set of actions), and\n",
            "observation (new information).\n",
            "Actions are limited to predefined options like search, lookup, and finish.\n",
            "The LLM goes through cycles of thought, action, and observation until it determines the answer.\n",
            "Instructions are provided to define the allowed actions and provide guidance to the LLM.\n",
            "\n",
            "LangChain\n",
            "LangChain provides modular components for working with LLMs in applications.\n",
            "It includes prompt templates for various use cases, memory to store LLM interactions, and tools for\n",
            "working with external datasets and APIs.\n",
            "Pre-built chains optimised for different use cases are available for quick deployment.\n",
            "Agents, such as PAL and ReAct, can be incorporated into chains to plan and execute actions.\n",
            "LangChain is actively developed with new features being added, allowing for fast prototyping and\n",
            "deployment.\n",
            "\n",
            "Other Considerations\n",
            "The ability of the LLM to reason and plan actions depends on its scale.\n",
            "Larger models are generally more suitable for advanced prompting techniques like PAL and ReAct.\n",
            "Smaller models may struggle with highly structured prompts and may require additional fine-tuning.\n",
            "Starting with a large model and collecting user data in deployment can potentially train a smaller,\n",
            "fine-tuned model for better performance.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "LLM Application Architecture\n",
            "\n",
            "Users\n",
            "\n",
            "Systems\n",
            "\n",
            "Application Interface - Web, Mobile App, APIs etc.\n",
            "\n",
            "Tools & Frameworks like LangChain, ModelHub etc.\n",
            "Information Sources\n",
            "Documents\n",
            "Databses\n",
            "Web\n",
            "\n",
            "LLM Models\n",
            "(Optimised)\n",
            "\n",
            "Generated Outputs\n",
            "Feedback on Outputs\n",
            "\n",
            "Infrastructure - for training, fine-tuning, serving, application components etc.\n",
            "\n",
            "Key Components of LLM Powered Applications\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Responsible AI\n",
            "Toxicity: Toxic language or content that can be harmful or discriminatory towards certain groups.\n",
            "Mitigation strategies include curating training data, training guardrail models to filter out unwanted\n",
            "content, providing guidance to human annotators, and ensuring diversity among annotators.\n",
            "Hallucinations: False or baseless statements generated by the model due to gaps in training data. To\n",
            "mitigate this, educate users about the technology's limitations, augment models with independent and\n",
            "verified sources, attribute generated output to training data, and define intended and unintended use\n",
            "cases.\n",
            "Intellectual Property: The risk of using data returned by models that may plagiarise or infringe on\n",
            "existing work. Addressing this challenge requires a combination of technological advancements, legal\n",
            "mechanisms, governance systems, and approaches like machine unlearning and content\n",
            "filtering/blocking.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Generative AI Project Lifecycle Cheatsheet\n",
            "Training Duration\n",
            "\n",
            "Customisation\n",
            "\n",
            "Objective\n",
            "\n",
            "Expertise\n",
            "\n",
            "Pre-training\n",
            "\n",
            "Days/ Weeks/ Months\n",
            "\n",
            "Architecture\n",
            "Size\n",
            "Vocabulary\n",
            "Context Window\n",
            "Training Data\n",
            "\n",
            "Next token\n",
            "preditction\n",
            "\n",
            "High\n",
            "\n",
            "Prompt\n",
            "Engineering\n",
            "\n",
            "Not required\n",
            "\n",
            "Only prompt\n",
            "customisation\n",
            "\n",
            "Increase task\n",
            "performance\n",
            "\n",
            "Low\n",
            "\n",
            "Fine tuning\n",
            "/ Prompt\n",
            "tuning\n",
            "\n",
            "Minutes/ Hours\n",
            "\n",
            "Task specific\n",
            "tuning\n",
            "Domain specific\n",
            "data\n",
            "Update model /\n",
            "adapter weights\n",
            "\n",
            "Increase task\n",
            "performance\n",
            "\n",
            "Medium\n",
            "\n",
            "RLHF/ RLAIF\n",
            "\n",
            "Minutes/ Hours\n",
            "\n",
            "Train reward\n",
            "model [HHH\n",
            "goals]\n",
            "Update model /\n",
            "adapter weights\n",
            "\n",
            "Increase\n",
            "alignment with\n",
            "human\n",
            "preferences\n",
            "\n",
            "Medium - High\n",
            "\n",
            "Reduce model\n",
            "size\n",
            "Faster inference\n",
            "\n",
            "Increase\n",
            "inference\n",
            "performance\n",
            "\n",
            "Medium\n",
            "\n",
            "+ data collection for\n",
            "reward model\n",
            "\n",
            "Compression/ Minutes/ Hours\n",
            "Optimisation/\n",
            "Deployment\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "Acknowledgements\n",
            "Ever since the transformers architecture gained popularity, large language models have been infocus. From the\n",
            "era of skepticism in 2018/2019 to the explosive hype with the release of chatGPT in\n",
            "2022, the evolution has been nothing short of magical.\n",
            "Up until recently, the access to the knowledge was also largely inaccessible for regular people. This\n",
            "course by deeplearning.ai and AWS is another step in the democratisation of this technology. Like\n",
            "other courses by deeplearning.ai in the LLM and Generative AI space, this course is simple, practical\n",
            "and useful.\n",
            "I'd like to thank Coursera for being the influencer and contributor in my lifelong learning.\n",
            "Dr Andrew Ng, for making learning ML and data science for developers and enthusiasts\n",
            "unintimidating, simple and practical.\n",
            "The team at deeplearning.ai for making yet another course with such fine detail and\n",
            "practical labs.\n",
            "Antje Barth, Chris Fregly, Shelbee Eigenbrode and Mike Chambers for teaching me this very\n",
            "special course.\n",
            "All my colleagues and friends who endeavour to learn, discover and apply technology\n",
            "everyday in their effort to make the world a better place.\n",
            "With Lots of Love,\n",
            "\n",
            "Kim\n",
            "\n",
            "I talk about :\n",
            "\n",
            "#AI #MachineLearning #DataScience\n",
            "#GenerativeAI #DataProducts #Analytics\n",
            "#LLMs #Technology #Education #EthicalAI\n",
            "\n",
            "Let's connect:\n",
            "\n",
            "Nitesh\n",
            "\n",
            " ' metadata={'source': '/content/LLMs.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLa_U7eghmld",
        "outputId": "15bf83f8-d735-4faa-f23e-56409f48b483"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1362, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2450, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1153, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1436, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1163, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu0vGIq_iwYE",
        "outputId": "fc1fe7d3-9c09-4ef1-ac4c-30a53524a64c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaxORxKSiyDy",
        "outputId": "2f44a909-00a5-49ba-dda4-7485b98d0b35"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/LLMs.txt'}, page_content='Generative AI with Large\\nLanguage Models.\\nCourse Notes : July, 2023\\n\\nGenerative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of\\napplications.\\n\"LLMs can be, generally, thought of as a next word prediction model\"\\n\\nPART 1\\nLLM Pre-Training\\n\\nPART 2\\nLLM Fine Tuning\\n\\nPART 3\\nRLHF & Application\\n\\nPart 1\\nWhat is an LLM?\\n\\nPage 1\\n\\nWhat are the Use Cases for application of LLMs?\\n\\nPage 2\\n\\nWhat are Transformers? How was text generation done before Transformers? Transformer Architecture.\\n\\nPage 2\\n\\nHow does a Transformer generate Text?\\n\\nPage 4\\n\\nWhat is a Prompt?\\n\\nPage 5\\n\\nGenerative AI Project Life Cycle.\\n\\nPage 7\\n\\nHow do you pre-train Large Language Models?\\n\\nPage 8\\n\\nChallenges with pre-training LLMs.\\n\\nPage 9\\n\\nWhat is the optimal configuration for pre-training LLMs?\\n\\nPage 11\\n\\nWhen is pre-training useful?\\n\\nPage 12')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "F-E4u9Ktizp7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5_-YTOzi4pr",
        "outputId": "68109760-ca75-4554-f2e9-8af9ad2fb073"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "_O5ms2XEi_Gb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How was text generation done before Transformers?\"\n",
        "docs = db.similarity_search(query)"
      ],
      "metadata": {
        "id": "bRctQ2_fjDTi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhnQH7tPjLEy",
        "outputId": "5b903bef-1b4a-40c8-977c-aba87a9a2f2d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How was text generation done before Transformers?\n",
            "Before the arrival of transformers, text generation tasks were accomplished by Recurrent Neural\n",
            "Networks (RNNs).\n",
            "The next word was predicted looking at the previous few words. The more the number of previous\n",
            "words, the larger was the computational requirement of the RNN.\n",
            "The prediction wasn't great. The reason was the design of looking only at a few previous words.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "I took my money to the bank.\n",
            "HOMONYMS\n",
            "\n",
            "The teacher taught the student with the book\n",
            "\n",
            "River Bank?\n",
            "Financial Bank?\n",
            "\n",
            "SYNTACTIC\n",
            "AMBIGUITY\n",
            "\n",
            "Did teacher teach with the book?\n",
            "Was it a student with the book?\n",
            "\n",
            "TRANSFORMERS ARE ABLE TO PAY ATTENTION TO THE MEANING OF THE WORDS\n",
            "TRANSFORMERS SCALE EFFICIENTLY\n",
            "TRANSFORMERS CAN PROCESS DATA PARALLELLY\n",
            "\n",
            "What is Attention?\n",
            "\n",
            "Transformers supersede all previous natural language architectures because of their ability to 'pay attention'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is an LLM?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMDLwl7K-j3S",
        "outputId": "de3a1ef5-7f4d-4436-a57a-00c28b4366c3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is an LLM?\n",
            "LLMs are machine learning models that have learned from massive datasets of human-generated\n",
            "content, finding statistical patterns to replicate human-like abilities.\n",
            "Foundation models, also known as base models, have been trained on trillions of words for weeks or\n",
            "months using extensive compute power. These models have billions of parameters, which represent\n",
            "their memory and enable sophisticated tasks.\n",
            "Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code\n",
            "syntax, you provide natural language prompts to the models.\n",
            "When you pass a prompt to the model, it predicts the next words and generates a completion. This\n",
            "process is known as inference.\n",
            "\n",
            "Nitesh\n",
            "\n",
            " GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "PROMPT\n",
            "\n",
            "MODEL\n",
            "\n",
            "COMPLETION\n",
            "\n",
            "Where is Ganymede located in the\n",
            "\n",
            "Where is Ganymede located in the\n",
            "\n",
            "solar system?\n",
            "\n",
            "solar system?\n",
            "\n",
            "LLM\n",
            "CONTEXT WINDOW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a Prompt?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U-omyyzAEuC",
        "outputId": "8f20353c-ad55-45cd-e5b3-99fe27c0aba3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is a Prompt?\n",
            "The natural language instruction in which we interact with an LLM is called a Prompt. The construction\n",
            "of prompts is called Prompt Engineering.\n",
            "The inferencing that an LLM does and completes the instruction given in the prompt is called 'in\n",
            "context learning'\n",
            "The ability of the LLM to respond to the instruction in the prompt without any example is called 'Zero\n",
            "Shot Learning'\n",
            "When a single example is provided, it's called 'One Shot Learning'\n",
            "If more than one examples in provided, it's called 'Few Shot Learning'\n",
            "Context Window, or the maximum number of tokens that an LLM can provide and inference on, is\n",
            "critical in the Zero/One/Few Shot Learning\n",
            "\n",
            "ZERO SHOT LEARNING\n",
            "\n",
            "ONE SHOT LEARNING\n",
            "\n",
            "FEW SHOT LEARNING\n",
            "\n",
            "Classify this review :\n",
            "\n",
            "Classify this review :\n",
            "\n",
            "Classify this review :\n",
            "\n",
            "I loved this movie!\n",
            "\n",
            "I loved this movie!\n",
            "\n",
            "I loved this movie!\n",
            "\n",
            "Sentiment :\n",
            "\n",
            "Sentiment : Positive\n",
            "\n",
            "Sentiment : Positive\n",
            "\n",
            "Classify this review:\n",
            "\n",
            "Classify this review:\n",
            "\n",
            "I don't like this chair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Instruction Fine Tuning?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUEx-NgTGkSc",
        "outputId": "ce39ec7e-f196-4526-857b-f1d972fbbc0e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Instruction Fine Tuning?\n",
            "Through in context learning, or prompting, only a certain level of performance can be achieved.\n",
            "Few shot learning might not work for smaller LLMs and it also takes up a lot of space in the context\n",
            "window.\n",
            "Fine Tuning is a supervised learning process, where you take a labelled dataset of prompt-completion\n",
            "pairs to adjust the weights of an LLM.\n",
            "Instruction Fine Tuning is a strategy where the LLM is trained on examples of Instructions and how the\n",
            "LLM should respond to those instructions. Instruction Fine Tuning leads to improved performance on\n",
            "the instruction task.\n",
            "Full Fine Tuning is where all the LLM parameters are updated. It requires enough memory to store and\n",
            "process all the gradients and other components.\n",
            "\n",
            "Base Model\n",
            "\n",
            "trained on\n",
            "\n",
            "Task Specific Examples\n",
            "\n",
            "results in\n",
            "\n",
            "PROMPT [....], COMPLETION[....]\n",
            "PROMPT [....], COMPLETION[....]\n",
            "PROMPT [....], COMPLETION[....]\n",
            "......\n",
            "PROMPT [....], COMPLETION[....]\n",
            "\n",
            "Pre-trained\n",
            "LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is your model?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duylp6UU-8XV",
        "outputId": "38664023-f202-40c6-b9c8-77cd42a995dc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
            "\n",
            "Course Notes : July, 2023\n",
            "\n",
            "How to optimise and deploy LLMs for inferencing?\n",
            "Integrating a language model into applications requires considering factors like model speed, compute\n",
            "budget, and trade-offs between performance and speed/storage.\n",
            "Additional considerations include model interaction with external data or applications and\n",
            "determining the intended application or API interface.\n",
            "\n",
            "Optimisation techniques\n",
            "Distillation\n",
            "\n",
            "Post-training Quantisation\n",
            "\n",
            "Pruning\n",
            "\n",
            "FP32\n",
            "32 Bit Floating Point\n",
            "\n",
            "Teacher\n",
            "LLM\n",
            "\n",
            "Range\n",
            "3e-38 to 3e+38\n",
            "\n",
            "Original\n",
            "LLM\n",
            "\n",
            "FP16 | BFLOAT16 | INT8\n",
            "16 Bit Floating Point\n",
            "8 Bit Integer\n",
            "\n",
            "Pruned\n",
            "LLM\n",
            "\n",
            "Student\n",
            "LLM\n",
            "Use larger teacher model\n",
            "to train smaller student\n",
            "model\n",
            "Use student model for\n",
            "inferencing in\n",
            "applications\n",
            "Temperature parameter\n",
            "is used to generate soft\n",
            "and hard predictions.\n",
            "In practice, distillation is\n",
            "effecting in encoder only\n",
            "models like BERT\n"
          ]
        }
      ]
    }
  ]
}